{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the data\n",
    "\n",
    "The dataset is retrieved from the website http://data.vision.ee.ethz.ch/cvl/food-101.tar.gz using as a reference the following research paper\n",
    "\n",
    "\n",
    "> Lukas Bossard, Matthieu Guillaumin, Luc Van Gool - Food-101 – Mining Discriminative Components with Random Forests\n",
    "\n",
    "The Food-101 data set consists of images from Foodspotting [1]. Any use beyond\n",
    "   scientific fair use must be negociated with the respective picture owners\n",
    "   according to the Foodspotting terms of use [2].\n",
    "\n",
    "[1] http://www.foodspotting.com/\n",
    "[2] http://www.foodspotting.com/terms/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing essential modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from os import path\n",
    "import time\n",
    "from random import seed, choice\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deleting the data folder in case a cleaning is required\n",
    "#shutil.rmtree(\"../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-03-07 08:30:11--  http://data.vision.ee.ethz.ch/cvl/food-101.tar.gz\n",
      "Resolving data.vision.ee.ethz.ch (data.vision.ee.ethz.ch)... 129.132.52.162\n",
      "Connecting to data.vision.ee.ethz.ch (data.vision.ee.ethz.ch)|129.132.52.162|:80... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://data.vision.ee.ethz.ch/cvl/food-101.tar.gz [following]\n",
      "--2020-03-07 08:30:12--  https://data.vision.ee.ethz.ch/cvl/food-101.tar.gz\n",
      "Connecting to data.vision.ee.ethz.ch (data.vision.ee.ethz.ch)|129.132.52.162|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4996278331 (4.7G) [application/x-gzip]\n",
      "Saving to: ‘../data/food-101.tar.gz’\n",
      "\n",
      "../data/food-101.ta 100%[===================>]   4.65G  39.2MB/s    in 2m 0s   \n",
      "\n",
      "2020-03-07 08:32:12 (39.6 MB/s) - ‘../data/food-101.tar.gz’ saved [4996278331/4996278331]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#the data will be downloaded and automatically extracted to the data folder ../data/food-101/images\n",
    "%mkdir ../data\n",
    "!wget -O ../data/food-101.tar.gz http://data.vision.ee.ethz.ch/cvl/food-101.tar.gz\n",
    "!tar -zxf ../data/food-101.tar.gz -C ../data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organise train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dividing into train and test set using the json metadata \n",
    "\n",
    "metafolder = \"../data/food-101/meta/\"\n",
    "train_meta = pd.read_json(path_or_buf = metafolder + \"train.json\")\n",
    "test_meta = pd.read_json(path_or_buf = metafolder + \"test.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Organising metdatada for training, testing and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected categories : ramen, carrot_cake, beef_carpaccio, strawberry_shortcake, escargots\n"
     ]
    }
   ],
   "source": [
    "#organising metadata for training, testing and validation\n",
    "validation_split = 0.2\n",
    "val_split_idx = int(np.floor(train_meta.shape[0]*validation_split))\n",
    "\n",
    "#folder with all the food images\n",
    "data_dir = \"../data/food-101/images/\"\n",
    "folders_sorted = sorted(os.listdir(data_dir))\n",
    "\n",
    "#number of categories to randomly select\n",
    "nc = 5\n",
    "\n",
    "#selecting a randomn subset of categories\n",
    "seed(42)\n",
    "\n",
    "selection = []\n",
    "while len(selection) < nc:\n",
    "    pick = choice(folders_sorted)\n",
    "    if pick not in set(selection):\n",
    "        selection.append(pick)\n",
    "        \n",
    "print(\"Selected categories : {}\".format(', '.join(map(str, selection))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000 images used for training\n",
      "750 images used for validation\n",
      "1250 images used for testing\n"
     ]
    }
   ],
   "source": [
    "#create folder with data to upload to s3\n",
    "%mkdir ../data/s3_train_test_data\n",
    "%mkdir ../data/s3_train_test_data/train_img \n",
    "%mkdir ../data/s3_train_test_data/valid_img\n",
    "%mkdir ../data/s3_train_test_data/test_img\n",
    "\n",
    "train_meta = train_meta[selection].iloc[:train_meta.shape[0] - val_split_idx]\n",
    "\n",
    "valid_meta = train_meta[selection].iloc[train_meta.shape[0] - val_split_idx:]\n",
    "\n",
    "test_meta = test_meta[selection]\n",
    "\n",
    "#Setting train, validation and test set target folder\n",
    "#target folder - train\n",
    "trainfolder = \"../data/s3_train_test_data/train_img/\"\n",
    "\n",
    "#target folder - validation\n",
    "validfolder = \"../data/s3_train_test_data/valid_img/\"\n",
    "\n",
    "#target folder -test\n",
    "testfolder = \"../data/s3_train_test_data/test_img/\"\n",
    "\n",
    "print(\"{} images used for training\".format(train_meta.shape[0]*train_meta.shape[1]))\n",
    "print(\"{} images used for validation\".format(valid_meta.shape[0]*valid_meta.shape[1]))\n",
    "print(\"{} images used for testing\".format(test_meta.shape[0]*test_meta.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dividing into train and test set using the json metadata \n",
    "\n",
    "def organise_files_from_df(df, datafolder, datatarget):\n",
    "    \"\"\"\n",
    "    This function moves files contained in a folder (datafolder) to a target path (datatarget),\n",
    "    based on the information contained on a dataframe (df) where each column corresponds to a \n",
    "    class name (sub-folder). Every column of the dataset contains a list of filenames to be moved.\n",
    "    \"\"\"\n",
    "    \n",
    "    #creating target folder\n",
    "    if not path.exists(datatarget):\n",
    "        os.mkdir(datatarget)\n",
    "    \n",
    "    #iterating through dataframe columns ( =  labels)\n",
    "    for label in list(df.columns):\n",
    "        \n",
    "        #create folder\n",
    "        foldername = datatarget + str(label)\n",
    "        \n",
    "        if not path.exists(foldername):\n",
    "            os.mkdir(foldername)\n",
    "        \n",
    "        #move each file\n",
    "        for file in list(df[label]):\n",
    "            \n",
    "            fileoriginal =  datafolder + file + \".jpg\"\n",
    "            filetarget = datatarget +\"/\" + file + \".jpg\"\n",
    "            \n",
    "            try:\n",
    "                if not path.exists(filetarget):\n",
    "                    shutil.copyfile(fileoriginal, filetarget)\n",
    "\n",
    "            except FileNotFoundError:\n",
    "                print(\"File {} not found!\".format(file))\n",
    "                pass\n",
    "\n",
    "#origin folder\n",
    "imagefolder = \"../data/food-101/images/\"\n",
    "\n",
    "\n",
    "organise_files_from_df(train_meta, imagefolder, trainfolder)\n",
    "\n",
    "organise_files_from_df(valid_meta, imagefolder, validfolder)\n",
    "\n",
    "organise_files_from_df(test_meta, imagefolder, testfolder)\n",
    "\n",
    "#to delete the origin folder, uncomment the line below\n",
    "#shutil.rmtree(imagefolder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data to S3\n",
    "\n",
    ">The below cells load in some AWS SageMaker libraries, starts a SageMaker session and creates a default bucket. After creating this bucket, it upload the locally stored data to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "# session and role\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# create an S3 bucket\n",
    "bucket = sagemaker_session.default_bucket()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Upload training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data uploaded to s3 after 361.97146677970886 seconds\n"
     ]
    }
   ],
   "source": [
    "prefix = \"food-classifier\"\n",
    "datafolder = \"../data/s3_train_test_data\"\n",
    "# upload all data to S3\n",
    "\n",
    "#this is slow!\n",
    "start = time.time()\n",
    "input_data = sagemaker_session.upload_data(path=datafolder, bucket=bucket, key_prefix=prefix)\n",
    "end = time.time()\n",
    "\n",
    "print(\"Data uploaded to s3 after {} seconds\".format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All good!\n"
     ]
    }
   ],
   "source": [
    "# check that data is in S3 bucket\n",
    "empty_check = []\n",
    "for obj in boto3.resource('s3').Bucket(bucket).objects.all():\n",
    "    empty_check.append(obj.key)\n",
    "    #print(obj.key)\n",
    "\n",
    "assert len(empty_check) !=0, 'S3 bucket is empty.'\n",
    "print('All good!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Checking model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorchvision.models\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mmodels\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m#importing pretrained ResNet for transfer learning\u001b[39;49;00m\n",
      "ResNetTransfer = models.resnet50(pretrained=\u001b[36mTrue\u001b[39;49;00m) \n"
     ]
    }
   ],
   "source": [
    "!pygmentize pytorch_source/model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Checking train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch.optim\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36moptim\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch.nn\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnn\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch.utils.data\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtorchvision\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m datasets\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorchvision.transforms\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mtransforms\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch.optim\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36moptim\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtorch.optim.lr_scheduler\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m ReduceLROnPlateau\n",
      "\n",
      "\u001b[37m# imports the model in model.py by name\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mmodel\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m ResNetTransfer\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmodel_fn\u001b[39;49;00m(model_dir):\n",
      "    \u001b[33m\"\"\"Load the PyTorch model from the `model_dir` directory.\"\"\"\u001b[39;49;00m\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mLoading model.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[37m# First, load the parameters used to create the model.\u001b[39;49;00m\n",
      "    model_info = {}\n",
      "    model_info_path = os.path.join(model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel_info.pth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(model_info_path, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\n",
      "        model_info = torch.load(f)\n",
      "\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mmodel_info: {}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(model_info))\n",
      "\n",
      "    \u001b[37m# Determine the device and construct the model.\u001b[39;49;00m\n",
      "    device = torch.device(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m torch.cuda.is_available() \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mUsing device {}.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(device))\n",
      "\n",
      "    model = ResNetTransfer\n",
      "\n",
      "    \u001b[37m#freezing the parameters\u001b[39;49;00m\n",
      "    \u001b[34mfor\u001b[39;49;00m param \u001b[35min\u001b[39;49;00m model.parameters():\n",
      "        param.requires_grad = \u001b[36mFalse\u001b[39;49;00m\n",
      "    model.fc = nn.Linear(model.fc.in_features, model_info[\u001b[33m\"\u001b[39;49;00m\u001b[33mn_classes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "\n",
      "    \u001b[37m# Load the stored model parameters.\u001b[39;49;00m\n",
      "    model_path = os.path.join(model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel.pth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(model_path, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\n",
      "        model.load_state_dict(torch.load(f))\n",
      "\n",
      "    \u001b[37m# set to eval mode, could use no_grad\u001b[39;49;00m\n",
      "    model.to(device).eval()\n",
      "\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mDone loading model.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \u001b[34mreturn\u001b[39;49;00m model\n",
      "\n",
      "\n",
      "\u001b[37m# Gets prepared training data for the Dataloaders\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_get_train_data_loader\u001b[39;49;00m(img_short_side_resize, img_input_size, norm_mean, norm_std,\n",
      "                           shuffle, num_workers, batch_size, datadir, trainfolder, validfolder, testfolder):\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mGet train data loader.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "    transform_train = transforms.Compose([\n",
      "                    transforms.Resize(img_short_side_resize),\n",
      "                    transforms.ColorJitter(brightness=\u001b[34m0.2\u001b[39;49;00m, contrast=\u001b[34m0.2\u001b[39;49;00m, saturation=\u001b[34m0.2\u001b[39;49;00m, hue=\u001b[34m0.1\u001b[39;49;00m),\n",
      "                    transforms.RandomHorizontalFlip(),\n",
      "                    transforms.RandomResizedCrop(img_input_size, scale=(\u001b[34m0.08\u001b[39;49;00m,\u001b[34m1\u001b[39;49;00m), ratio=(\u001b[34m1\u001b[39;49;00m,\u001b[34m1\u001b[39;49;00m)), \n",
      "                    transforms.ToTensor(),\n",
      "                    transforms.Normalize(mean = norm_mean, std = norm_std)])\n",
      "    transform_test = transforms.Compose([\n",
      "                        transforms.Resize(img_input_size),  \n",
      "                        transforms.FiveCrop(img_input_size),\n",
      "                        transforms.Lambda(\u001b[34mlambda\u001b[39;49;00m crops: torch.stack([transforms.Compose([\n",
      "                        transforms.ToTensor(),\n",
      "                        transforms.Normalize(mean = norm_mean, std = norm_std)])(crop) \u001b[34mfor\u001b[39;49;00m crop \u001b[35min\u001b[39;49;00m crops]))])\n",
      "\n",
      "    train_data = datasets.ImageFolder(datadir + trainfolder, transform_train)\n",
      "    valid_data = datasets.ImageFolder(datadir + validfolder, transform_test)\n",
      "    test_data = datasets.ImageFolder(datadir + testfolder, transform_test)\n",
      "\n",
      "    \u001b[37m# Create the data loaders\u001b[39;49;00m\n",
      "    data = {\u001b[33m\"\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m : train_data, \u001b[33m\"\u001b[39;49;00m\u001b[33mval\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:valid_data, \u001b[33m\"\u001b[39;49;00m\u001b[33mtest\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:test_data}\n",
      "\n",
      "    train_loader = torch.utils.data.DataLoader(data[\u001b[33m\"\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], batch_size=batch_size, num_workers=num_workers, shuffle=shuffle, pin_memory=\u001b[36mTrue\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[37m#### --- NOTE on num_workers if using 5crop and batch_size for testing --- ###\u001b[39;49;00m\n",
      "    \u001b[37m# If using the 5crop test time augmentation, num_workers = 0 (an error is raised otherwise) \u001b[39;49;00m\n",
      "    \u001b[37m# batch_size needs to be reduced during testing due to memory requirements\u001b[39;49;00m\n",
      "    valid_loader = torch.utils.data.DataLoader(data[\u001b[33m\"\u001b[39;49;00m\u001b[33mval\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], batch_size=\u001b[36mint\u001b[39;49;00m(np.floor(batch_size/\u001b[34m5\u001b[39;49;00m)), num_workers=\u001b[34m0\u001b[39;49;00m, shuffle=shuffle, pin_memory=\u001b[36mTrue\u001b[39;49;00m)\n",
      "    test_loader = torch.utils.data.DataLoader(data[\u001b[33m\"\u001b[39;49;00m\u001b[33mtest\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], batch_size=\u001b[36mint\u001b[39;49;00m(np.floor(batch_size/\u001b[34m5\u001b[39;49;00m)), num_workers=\u001b[34m0\u001b[39;49;00m, shuffle=shuffle, pin_memory=\u001b[36mTrue\u001b[39;49;00m)\n",
      "\n",
      "    loaders_transfer = {\u001b[33m\"\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m : train_loader, \u001b[33m\"\u001b[39;49;00m\u001b[33mval\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:valid_loader, \u001b[33m\"\u001b[39;49;00m\u001b[33mtest\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: test_loader}\n",
      "\n",
      "    \u001b[34mreturn\u001b[39;49;00m loaders_transfer\n",
      "\n",
      "\n",
      "\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtime\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mdatetime\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtrain_epoch\u001b[39;49;00m(model,train_loader,optimizer,criterion,device):\n",
      "    \u001b[33m\"\"\"\u001b[39;49;00m\n",
      "\u001b[33m    train steps at each epoch\u001b[39;49;00m\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\n",
      "    train_loss = \u001b[34m0.0\u001b[39;49;00m\n",
      "    \n",
      "    model.train()\n",
      "    \n",
      "    \u001b[34mfor\u001b[39;49;00m batch_idx, (data, target) \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(train_loader):\n",
      "        \n",
      "        data, target = data.to(device), target.to(device) \u001b[37m# move to GPU\u001b[39;49;00m\n",
      "        \n",
      "        optimizer.zero_grad() \u001b[37m# set gradients to 0\u001b[39;49;00m\n",
      "        \n",
      "        output = model(data) \u001b[37m# get output\u001b[39;49;00m\n",
      "        \n",
      "        loss = criterion(output, target) \u001b[37m# calculate loss\u001b[39;49;00m\n",
      "        train_loss += loss.item() * data.size(\u001b[34m0\u001b[39;49;00m)\n",
      "        \n",
      "        loss.backward() \u001b[37m# calculate gradients\u001b[39;49;00m\n",
      "        \n",
      "        optimizer.step() \u001b[37m# take step\u001b[39;49;00m\n",
      "        \n",
      "    train_loss = train_loss / \u001b[36mlen\u001b[39;49;00m(train_loader.dataset)\n",
      "    \u001b[34mreturn\u001b[39;49;00m model, train_loss\n",
      "        \n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mvalid_epoch\u001b[39;49;00m(model, valid_loader, criterion, device, fivecrop):\n",
      "    \u001b[33m\"\"\"\u001b[39;49;00m\n",
      "\u001b[33m    validation prediction steps at each epoch\u001b[39;49;00m\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\n",
      "    valid_loss = \u001b[34m0.0\u001b[39;49;00m\n",
      "    \n",
      "    model.eval()\n",
      "    \n",
      "    \u001b[34mwith\u001b[39;49;00m torch.no_grad():\n",
      "        \u001b[34mfor\u001b[39;49;00m data, target \u001b[35min\u001b[39;49;00m valid_loader:\n",
      "            \n",
      "            data, target = data.to(device), target.to(device) \u001b[37m# move to GPU\u001b[39;49;00m\n",
      "            \n",
      "            \u001b[37m# if we do test time augmentation with 5crop we'll have an extra dimension in our tensor\u001b[39;49;00m\n",
      "            \u001b[34mif\u001b[39;49;00m fivecrop == \u001b[33m\"\u001b[39;49;00m\u001b[33mmean\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\n",
      "                bs, ncrops, c, h, w = data.size()\n",
      "                output = model(data.view(-\u001b[34m1\u001b[39;49;00m, c, h, w)) \u001b[37m# fuse batch size and ncrops\u001b[39;49;00m\n",
      "                output = output.view(bs, ncrops, -\u001b[34m1\u001b[39;49;00m).mean(\u001b[34m1\u001b[39;49;00m)\n",
      "            \u001b[34melif\u001b[39;49;00m fivecrop == \u001b[33m\"\u001b[39;49;00m\u001b[33mmax\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\n",
      "                bs, ncrops, c, h, w = data.size()\n",
      "                output = model(data.view(-\u001b[34m1\u001b[39;49;00m, c, h, w)) \u001b[37m# fuse batch size and ncrops\u001b[39;49;00m\n",
      "                output = output.view(bs, ncrops, -\u001b[34m1\u001b[39;49;00m).max(\u001b[34m1\u001b[39;49;00m)[\u001b[34m0\u001b[39;49;00m]\n",
      "            \u001b[34melse\u001b[39;49;00m:\n",
      "                output = model(data)\n",
      "                \n",
      "            \u001b[37m## update the average validation loss\u001b[39;49;00m\n",
      "            loss = criterion(output, target)\n",
      "            valid_loss += loss.item() * data.size(\u001b[34m0\u001b[39;49;00m)\n",
      "            \n",
      "    valid_loss = valid_loss / \u001b[36mlen\u001b[39;49;00m(valid_loader.dataset) \n",
      "    \u001b[34mreturn\u001b[39;49;00m valid_loss\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtrain\u001b[39;49;00m(n_epochs, loaders, model, optimizer, criterion, device, path_model, fivecrop = \u001b[36mNone\u001b[39;49;00m, lr_scheduler = \u001b[36mNone\u001b[39;49;00m):\n",
      "    \u001b[33m\"\"\"\u001b[39;49;00m\n",
      "\u001b[33m    model training\u001b[39;49;00m\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\n",
      "    \n",
      "    \u001b[37m# initialize tracker for minimum validation loss\u001b[39;49;00m\n",
      "    valid_loss_min = np.Inf \n",
      "    train_loss = []\n",
      "    valid_loss = []\n",
      "    \n",
      "    time_start = time.time()\n",
      "    best_epoch = \u001b[34m0\u001b[39;49;00m\n",
      "    \n",
      "    \u001b[34mfor\u001b[39;49;00m epoch \u001b[35min\u001b[39;49;00m \u001b[36mrange\u001b[39;49;00m(\u001b[34m1\u001b[39;49;00m, n_epochs+\u001b[34m1\u001b[39;49;00m):\n",
      "        \n",
      "        time_start_epoch = time.time()  \n",
      "        \n",
      "        \u001b[37m# train current epoch\u001b[39;49;00m\n",
      "        model, train_loss_epoch = train_epoch(model, loaders[\u001b[33m\"\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], optimizer, criterion, device) \n",
      "        train_loss.append(train_loss_epoch)   \n",
      "        \n",
      "        \u001b[37m# validate current epoch\u001b[39;49;00m\n",
      "        valid_loss_epoch = valid_epoch(model,loaders[\u001b[33m\"\u001b[39;49;00m\u001b[33mval\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m],criterion,device,fivecrop)\n",
      "        \n",
      "        \u001b[37m# learning rate scheduler\u001b[39;49;00m\n",
      "        \u001b[34mif\u001b[39;49;00m lr_scheduler \u001b[35mis\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m \u001b[36mNone\u001b[39;49;00m:\n",
      "            lr_scheduler.step(valid_loss_epoch)\n",
      "        valid_loss.append(valid_loss_epoch)  \n",
      "        \n",
      "        \u001b[34mif\u001b[39;49;00m valid_loss_epoch <= valid_loss_min: \u001b[37m# save if validation loss is the lowest so far\u001b[39;49;00m\n",
      "            torch.save(model.state_dict(), path_model)\n",
      "            valid_loss_min = valid_loss_epoch \n",
      "            best_epoch = epoch\n",
      "            \n",
      "        \u001b[37m# print epoch stats\u001b[39;49;00m\n",
      "        currentDT = datetime.datetime.now()\n",
      "        exact_time =  \u001b[36mstr\u001b[39;49;00m(currentDT.hour) + \u001b[33m\"\u001b[39;49;00m\u001b[33m:\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m + \u001b[36mstr\u001b[39;49;00m(currentDT.minute) + \u001b[33m\"\u001b[39;49;00m\u001b[33m:\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m + \u001b[36mstr\u001b[39;49;00m(currentDT.second)\n",
      "        \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mEpoch {} done in {:.2f} seconds at {}. \u001b[39;49;00m\u001b[33m\\t\u001b[39;49;00m\u001b[33mTraining Loss: {:.3f} \u001b[39;49;00m\u001b[33m\\t\u001b[39;49;00m\u001b[33mValidation Loss: {:.3f}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format( \n",
      "            epoch,             \n",
      "            time.time() - time_start_epoch,\n",
      "            exact_time,\n",
      "            train_loss_epoch,\n",
      "            valid_loss_epoch\n",
      "            ))   \n",
      "        \n",
      "    \u001b[37m# print final statistics    \u001b[39;49;00m\n",
      "    \u001b[34mprint\u001b[39;49;00m(f\u001b[33m\"\u001b[39;49;00m\u001b[33m{n_epochs} epochs trained in {(time.time() - time_start):.3f} seconds. \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \n",
      "    \n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mBest model obtained at epoch {} with minimum validation loss : {:.3f}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(best_epoch, valid_loss_min))\n",
      "    \n",
      "    \u001b[37m# Load best config\u001b[39;49;00m\n",
      "    model.load_state_dict(torch.load(path_model))\n",
      "    \n",
      "    \u001b[34mreturn\u001b[39;49;00m model\n",
      "\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtest\u001b[39;49;00m(loaders, model, criterion, device):\n",
      "    \u001b[33m\"\"\"\u001b[39;49;00m\n",
      "\u001b[33m    test function\u001b[39;49;00m\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\n",
      "    \n",
      "    test_loss = \u001b[34m0.\u001b[39;49;00m\n",
      "    correct = \u001b[34m0.\u001b[39;49;00m\n",
      "    total = \u001b[34m0.\u001b[39;49;00m\n",
      "    \n",
      "    model.eval()\n",
      "    \n",
      "    \u001b[34mwith\u001b[39;49;00m torch.no_grad():\n",
      "        \n",
      "        \u001b[34mfor\u001b[39;49;00m batch_idx, (data, target) \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(loaders[\u001b[33m'\u001b[39;49;00m\u001b[33mtest\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]):\n",
      "            \n",
      "            data, target = data.to(device), target.to(device) \u001b[37m# move to GPU\u001b[39;49;00m\n",
      "            \n",
      "            bs, ncrops, c, h, w = data.size()\n",
      "            \n",
      "            \u001b[37m# forward pass: compute predicted outputs by passing inputs to the model\u001b[39;49;00m\n",
      "            output = model(data.view(-\u001b[34m1\u001b[39;49;00m, c, h, w)) \u001b[37m# fuse batch size and ncrops\u001b[39;49;00m\n",
      "            output = output.view(bs, ncrops, -\u001b[34m1\u001b[39;49;00m).mean(\u001b[34m1\u001b[39;49;00m)    \n",
      "            \n",
      "            loss = criterion(output, target) \u001b[37m# calculate the loss\u001b[39;49;00m\n",
      "            \n",
      "            test_loss = test_loss + ((\u001b[34m1\u001b[39;49;00m / (batch_idx + \u001b[34m1\u001b[39;49;00m)) * (loss.data - test_loss)) \u001b[37m# update average test loss \u001b[39;49;00m\n",
      "            \n",
      "            pred = output.data.max(\u001b[34m1\u001b[39;49;00m, keepdim=\u001b[36mTrue\u001b[39;49;00m)[\u001b[34m1\u001b[39;49;00m] \u001b[37m# convert output probabilities to predicted class\u001b[39;49;00m\n",
      "            \n",
      "            \u001b[37m# compare predictions to true label\u001b[39;49;00m\n",
      "            correct += np.sum(np.squeeze(pred.eq(target.data.view_as(pred))).cpu().numpy()) \n",
      "            total += data.size(\u001b[34m0\u001b[39;49;00m)            \n",
      "            \n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mTest Loss: {:.6f}\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(test_loss))\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33mTest Accuracy: \u001b[39;49;00m\u001b[33m%2d\u001b[39;49;00m\u001b[33m%%\u001b[39;49;00m\u001b[33m (\u001b[39;49;00m\u001b[33m%2d\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m%2d\u001b[39;49;00m\u001b[33m)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m % (\n",
      "        \u001b[34m100.\u001b[39;49;00m * correct / total, correct, total))\n",
      "\n",
      "\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m'\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\n",
      "    \n",
      "    \u001b[37m# Parameters settings\u001b[39;49;00m\n",
      "    parser = argparse.ArgumentParser()\n",
      "\n",
      "    \u001b[37m# SageMaker parameters\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--output-data-dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_OUTPUT_DATA_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--model-dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--data-dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CHANNEL_TRAINING\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    \n",
      "    \u001b[37m#Data preparation parameters\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--img_short_side_resize\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m256\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mI\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mResize to (default: 256)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--img_input_size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m224\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mI\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mImage input to ResNet (default: 224)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--shuffle\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mbool\u001b[39;49;00m, default=\u001b[36mTrue\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mI\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mDataloader shuffle (default: True)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--num_workers\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m16\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mI\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mnumber of workers in data preparation (default: 16)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--trainfolder\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m'\u001b[39;49;00m\u001b[33m/train_img\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mI\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mName of the folder containing training data (default: train_img)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--validfolder\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m'\u001b[39;49;00m\u001b[33m/valid_img\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mI\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mName of the folder containing validation data (default: valid_img)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--testfolder\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m'\u001b[39;49;00m\u001b[33m/test_img\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mI\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mName of the folder containing test data (default: test_img)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[37m# Training Parameters\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--batch_size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m64\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33minput batch size for training (default: 64)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--n_epochs\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m10\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mnumber of epochs to train (default: 10)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--seed\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m42\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mS\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mrandom seed (default: 42)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--lr\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m3e-4\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mLR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mlearning rate (default: 3e-4)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \n",
      "    \u001b[37m# Model Parameters\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--n_classes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m2\u001b[39;49;00m, metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mM\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33moutput dimension of the model (int - default: 2)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \n",
      "    \n",
      "    \u001b[37m# args holds all passed-in arguments\u001b[39;49;00m\n",
      "    args = parser.parse_args()\n",
      "\n",
      "    device = torch.device(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m torch.cuda.is_available() \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mUsing device {}.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(device))\n",
      "    \u001b[34mif\u001b[39;49;00m torch.cuda.is_available():\n",
      "        \u001b[34mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mUsing\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,torch.cuda.get_device_name(device))\n",
      "\n",
      "    torch.manual_seed(args.seed)\n",
      "\n",
      "    \u001b[37m# Load the training data.\u001b[39;49;00m\n",
      "    \u001b[37m#setting norm_mean and norm_std\u001b[39;49;00m\n",
      "    norm_mean = [\u001b[34m0.485\u001b[39;49;00m, \u001b[34m0.456\u001b[39;49;00m, \u001b[34m0.406\u001b[39;49;00m]\n",
      "    norm_std = [\u001b[34m0.229\u001b[39;49;00m, \u001b[34m0.224\u001b[39;49;00m, \u001b[34m0.225\u001b[39;49;00m]\n",
      "\n",
      "    train_test_data_loader = _get_train_data_loader(args.img_short_side_resize, \n",
      "                                          args.img_input_size,\n",
      "                                          norm_mean,\n",
      "                                          norm_std,\n",
      "                                          args.shuffle,\n",
      "                                          args.num_workers,\n",
      "                                          args.batch_size, \n",
      "                                          args.data_dir,\n",
      "                                          args.trainfolder,\n",
      "                                          args.validfolder,\n",
      "                                          args.testfolder)\n",
      "\n",
      "    \u001b[37m# Load the ResNet model\u001b[39;49;00m\n",
      "    model = ResNetTransfer\n",
      "\n",
      "    \u001b[37m#freezing the parameters\u001b[39;49;00m\n",
      "    \u001b[34mfor\u001b[39;49;00m param \u001b[35min\u001b[39;49;00m model.parameters():\n",
      "        param.requires_grad = \u001b[36mFalse\u001b[39;49;00m\n",
      "        \n",
      "    \u001b[37m# Replacing the last layer with a fully connected layer to retrain\u001b[39;49;00m\n",
      "    model.fc = nn.Linear(model.fc.in_features, args.n_classes) \n",
      "\n",
      "    \u001b[37m# Initialize the weights of the new layer\u001b[39;49;00m\n",
      "    nn.init.kaiming_normal_(model.fc.weight, nonlinearity=\u001b[33m'\u001b[39;49;00m\u001b[33mrelu\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[37m# Transfer to GPU \u001b[39;49;00m\n",
      "    model = model.to(device)\n",
      "\n",
      "    \u001b[37m## Optimizer and loss function for training\u001b[39;49;00m\n",
      "    criterion_transfer = nn.CrossEntropyLoss()\n",
      "    optimizer_transfer = optim.Adam(model.parameters(),args.lr) \n",
      "    scheduler_transfer = ReduceLROnPlateau(optimizer_transfer, \u001b[33m'\u001b[39;49;00m\u001b[33mmin\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, verbose = \u001b[36mTrue\u001b[39;49;00m, factor = \u001b[34m0.5\u001b[39;49;00m, patience = \u001b[34m7\u001b[39;49;00m)\n",
      "    \n",
      "\n",
      "    \u001b[37m# Trains the model \u001b[39;49;00m\n",
      "    model = train(args.n_epochs, \n",
      "          train_test_data_loader, \n",
      "          model, \n",
      "          optimizer_transfer, \n",
      "          criterion_transfer, \n",
      "          device, \n",
      "          os.path.join(args.model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel.pth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m), \n",
      "          fivecrop = \u001b[33m\"\u001b[39;49;00m\u001b[33mmean\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \n",
      "          lr_scheduler = scheduler_transfer)\n",
      "\n",
      "    \u001b[37m# Keep the keys of this dictionary as they are \u001b[39;49;00m\n",
      "    model_info_path = os.path.join(args.model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel_info.pth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(model_info_path, \u001b[33m'\u001b[39;49;00m\u001b[33mwb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\n",
      "        model_info = {\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33mn_classes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: args.n_classes,\n",
      "        }\n",
      "        torch.save(model_info, f)\n",
      "        \n",
      "    \u001b[37m# Save the model parameters\u001b[39;49;00m\n",
      "    model_path = os.path.join(args.model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel.pth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(model_path, \u001b[33m'\u001b[39;49;00m\u001b[33mwb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\n",
      "        torch.save(model.state_dict(), f)\n",
      "        \n",
      "        \n",
      "    test(train_test_data_loader, model, criterion_transfer, device)\n"
     ]
    }
   ],
   "source": [
    "!pygmentize pytorch_source/train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-eu-central-1-515611759963/food-classifier/'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "# S3 bucket to load\n",
    "bucket = \"sagemaker-eu-central-1-515611759963\"\n",
    "\n",
    "prefix = \"food-classifier\"\n",
    "\n",
    "train_test_folder = 's3://{}/{}/'.format(bucket, prefix)\n",
    "\n",
    "# session and role\n",
    "sagemaker_session = sagemaker.Session(default_bucket=bucket)\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "train_test_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Upload training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#datafolder = \"../data/s3_train_test_data\"\n",
    "# upload all data to S3\n",
    "\n",
    "#this is slow!\n",
    "#start = time.time()\n",
    "#input_data = sagemaker_session.upload_data(path=datafolder, bucket=bucket, key_prefix=prefix)\n",
    "#end = time.time()\n",
    "\n",
    "#print(\"Data uploaded to s3 after {} seconds\".format(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Create pytorch estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import a PyTorch wrapper\n",
    "from sagemaker.pytorch import PyTorch, PyTorchModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify an output path\n",
    "# prefix is specified above\n",
    "output_path = 's3://{}/{}'.format(bucket, prefix)\n",
    "\n",
    "# instantiate a pytorch estimator\n",
    "estimator = PyTorch(entry_point='train.py',\n",
    "                    source_dir='pytorch_source', \n",
    "                    role=role,\n",
    "                    framework_version= '1.1.0', #'1.3.1',\n",
    "                    train_instance_count=1,\n",
    "                    train_instance_type='ml.p2.xlarge',\n",
    "                    output_path=output_path,\n",
    "                    sagemaker_session=sagemaker_session,\n",
    "                    hyperparameters={\n",
    "                        'n_classes': nc + 1,  # num of classes for the fully connected layer at the end of the network (defined on the first cells and increased by 1 for pytorch counting)\n",
    "                        'n_epochs': 3,\n",
    "                        'img_short_side_resize':256,\n",
    "                        'img_input_size':224,\n",
    "                        'num_workers':16,\n",
    "                        'batch_size':64\n",
    "                    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-08 17:17:19 Starting - Starting the training job...\n",
      "2020-03-08 17:17:21 Starting - Launching requested ML instances...\n",
      "2020-03-08 17:18:18 Starting - Preparing the instances for training.........\n",
      "2020-03-08 17:19:37 Downloading - Downloading input data......\n",
      "2020-03-08 17:20:40 Training - Downloading the training image..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2020-03-08 17:21:00,643 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2020-03-08 17:21:00,668 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2020-03-08 17:21:01,293 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2020-03-08 17:21:01,512 sagemaker-containers INFO     Module train does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2020-03-08 17:21:01,512 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2020-03-08 17:21:01,512 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2020-03-08 17:21:01,512 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/usr/bin/python -m pip install . -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 1)) (1.16.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 2)) (0.25.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 3)) (1.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 4)) (0.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->-r requirements.txt (line 2)) (2.8.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->-r requirements.txt (line 2)) (2019.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision->-r requirements.txt (line 4)) (6.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision->-r requirements.txt (line 4)) (1.12.0)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: train\n",
      "  Running setup.py bdist_wheel for train: started\n",
      "  Running setup.py bdist_wheel for train: finished with status 'done'\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-h4sifoh1/wheels/35/24/16/37574d11bf9bde50616c67372a334f94fa8356bc7164af8ca3\u001b[0m\n",
      "\u001b[34mSuccessfully built train\u001b[0m\n",
      "\u001b[34mInstalling collected packages: train\u001b[0m\n",
      "\u001b[34mSuccessfully installed train-1.0.0\u001b[0m\n",
      "\u001b[34mYou are using pip version 18.1, however version 20.0.2 is available.\u001b[0m\n",
      "\u001b[34mYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[34m2020-03-08 17:21:03,288 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch_size\": 64,\n",
      "        \"n_epochs\": 3,\n",
      "        \"num_workers\": 16,\n",
      "        \"img_input_size\": 224,\n",
      "        \"n_classes\": 6,\n",
      "        \"img_short_side_resize\": 256\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"sagemaker-pytorch-2020-03-08-17-17-19-239\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-eu-central-1-515611759963/sagemaker-pytorch-2020-03-08-17-17-19-239/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"batch_size\":64,\"img_input_size\":224,\"img_short_side_resize\":256,\"n_classes\":6,\"n_epochs\":3,\"num_workers\":16}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-eu-central-1-515611759963/sagemaker-pytorch-2020-03-08-17-17-19-239/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch_size\":64,\"img_input_size\":224,\"img_short_side_resize\":256,\"n_classes\":6,\"n_epochs\":3,\"num_workers\":16},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"sagemaker-pytorch-2020-03-08-17-17-19-239\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-eu-central-1-515611759963/sagemaker-pytorch-2020-03-08-17-17-19-239/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--batch_size\",\"64\",\"--img_input_size\",\"224\",\"--img_short_side_resize\",\"256\",\"--n_classes\",\"6\",\"--n_epochs\",\"3\",\"--num_workers\",\"16\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH_SIZE=64\u001b[0m\n",
      "\u001b[34mSM_HP_N_EPOCHS=3\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_WORKERS=16\u001b[0m\n",
      "\u001b[34mSM_HP_IMG_INPUT_SIZE=224\u001b[0m\n",
      "\u001b[34mSM_HP_N_CLASSES=6\u001b[0m\n",
      "\u001b[34mSM_HP_IMG_SHORT_SIDE_RESIZE=256\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/usr/local/bin:/usr/lib/python36.zip:/usr/lib/python3.6:/usr/lib/python3.6/lib-dynload:/usr/local/lib/python3.6/dist-packages:/usr/lib/python3/dist-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/usr/bin/python -m train --batch_size 64 --img_input_size 224 --img_short_side_resize 256 --n_classes 6 --n_epochs 3 --num_workers 16\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "2020-03-08 17:20:59 Training - Training image download completed. Training in progress.\u001b[34mUsing device cuda.\u001b[0m\n",
      "\u001b[34mUsing Tesla K80\u001b[0m\n",
      "\u001b[34mGet train data loader.\u001b[0m\n",
      "\u001b[34mEpoch 1 done in 71.25 seconds at 17:22:26. #011Training Loss: 1.466 #011Validation Loss: 1.063\u001b[0m\n",
      "\u001b[34mEpoch 2 done in 71.19 seconds at 17:23:37. #011Training Loss: 1.033 #011Validation Loss: 0.762\u001b[0m\n",
      "\u001b[34mEpoch 3 done in 71.77 seconds at 17:24:49. #011Training Loss: 0.848 #011Validation Loss: 0.631\u001b[0m\n",
      "\u001b[34m3 epochs trained in 214.220 seconds. \u001b[0m\n",
      "\u001b[34mBest model obtained at epoch 3 with minimum validation loss : 0.631\u001b[0m\n",
      "\u001b[34mTest Loss: 0.555235\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mTest Accuracy: 87% (1099/1250)\u001b[0m\n",
      "\u001b[34m2020-03-08 17:25:52,499 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2020-03-08 17:26:03 Uploading - Uploading generated training model\n",
      "2020-03-08 17:26:49 Completed - Training job completed\n",
      "Training seconds: 432\n",
      "Billable seconds: 432\n"
     ]
    }
   ],
   "source": [
    "#estimator.fit({'training': input_data})\n",
    "\n",
    "estimator.fit({'training': train_test_folder}) #to use already loaded data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------!"
     ]
    }
   ],
   "source": [
    "#predictor = estimator.deploy(instance_type='ml.p2.xlarge', initial_instance_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Delete endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
