{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the data\n",
    "\n",
    "The dataset is retrieved from the website http://data.vision.ee.ethz.ch/cvl/food-101.tar.gz using as a reference the following research paper\n",
    "\n",
    "\n",
    "> Lukas Bossard, Matthieu Guillaumin, Luc Van Gool - Food-101 – Mining Discriminative Components with Random Forests\n",
    "\n",
    "The Food-101 data set consists of images from Foodspotting [1]. Any use beyond\n",
    "   scientific fair use must be negociated with the respective picture owners\n",
    "   according to the Foodspotting terms of use [2].\n",
    "\n",
    "[1] http://www.foodspotting.com/\n",
    "[2] http://www.foodspotting.com/terms/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing essential modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from os import path\n",
    "import time\n",
    "from random import seed, choice\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deleting the data folder in case a cleaning is required\n",
    "#shutil.rmtree(\"../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the data will be downloaded and automatically extracted to the data folder ../data/food-101/images\n",
    "%mkdir ../data\n",
    "!wget -O ../data/food-101.tar.gz http://data.vision.ee.ethz.ch/cvl/food-101.tar.gz\n",
    "!tar -zxf ../data/food-101.tar.gz -C ../data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organise train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dividing into train and test set using the json metadata \n",
    "\n",
    "metafolder = \"../data/food-101/meta/\"\n",
    "train_meta = pd.read_json(path_or_buf = metafolder + \"train.json\")\n",
    "test_meta = pd.read_json(path_or_buf = metafolder + \"test.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Organising metdatada for training, testing and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected categories : ramen, carrot_cake, beef_carpaccio, strawberry_shortcake, escargots\n"
     ]
    }
   ],
   "source": [
    "#organising metadata for training, testing and validation\n",
    "validation_split = 0.2\n",
    "val_split_idx = int(np.floor(train_meta.shape[0]*validation_split))\n",
    "\n",
    "#folder with all the food images\n",
    "data_dir = \"../data/food-101/images/\"\n",
    "folders_sorted = sorted(os.listdir(data_dir))\n",
    "\n",
    "#number of categories to randomly select\n",
    "nc = 5\n",
    "\n",
    "#selecting a randomn subset of categories\n",
    "seed(42)\n",
    "\n",
    "selection = []\n",
    "while len(selection) < nc:\n",
    "    pick = choice(folders_sorted)\n",
    "    if pick not in set(selection):\n",
    "        selection.append(pick)\n",
    "        \n",
    "print(\"Selected categories : {}\".format(', '.join(map(str, selection))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘../data/s3_train_data’: File exists\n",
      "mkdir: cannot create directory ‘../data/s3_train_data/train_img’: File exists\n",
      "mkdir: cannot create directory ‘../data/s3_train_data/valid_img’: File exists\n",
      "mkdir: cannot create directory ‘../data/test_img’: File exists\n",
      "3000 images used for training\n",
      "750 images used for validation\n",
      "1250 images used for testing\n"
     ]
    }
   ],
   "source": [
    "#create folder with data to upload to s3\n",
    "%mkdir ../data/s3_train_data\n",
    "%mkdir ../data/s3_train_data/train_img \n",
    "%mkdir ../data/s3_train_data/valid_img\n",
    "%mkdir ../data/test_img\n",
    "\n",
    "train_meta = train_meta[selection].iloc[:train_meta.shape[0] - val_split_idx]\n",
    "\n",
    "valid_meta = train_meta[selection].iloc[train_meta.shape[0] - val_split_idx:]\n",
    "\n",
    "test_meta = test_meta[selection]\n",
    "\n",
    "#Setting train, validation and test set target folder\n",
    "#target folder - train\n",
    "trainfolder = \"../data/s3_train_data/train_img/\"\n",
    "\n",
    "#target folder - validation\n",
    "validfolder = \"../data/s3_train_data/valid_img/\"\n",
    "\n",
    "#target folder -test\n",
    "testfolder = \"../data/test_img/\"\n",
    "\n",
    "print(\"{} images used for training\".format(train_meta.shape[0]*train_meta.shape[1]))\n",
    "print(\"{} images used for validation\".format(valid_meta.shape[0]*valid_meta.shape[1]))\n",
    "print(\"{} images used for testing\".format(test_meta.shape[0]*test_meta.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dividing into train and test set using the json metadata \n",
    "\n",
    "def organise_files_from_df(df, datafolder, datatarget):\n",
    "    \"\"\"\n",
    "    This function moves files contained in a folder (datafolder) to a target path (datatarget),\n",
    "    based on the information contained on a dataframe (df) where each column corresponds to a \n",
    "    class name (sub-folder). Every column of the dataset contains a list of filenames to be moved.\n",
    "    \"\"\"\n",
    "    \n",
    "    #creating target folder\n",
    "    if not path.exists(datatarget):\n",
    "        os.mkdir(datatarget)\n",
    "    \n",
    "    #iterating through dataframe columns ( =  labels)\n",
    "    for label in list(df.columns):\n",
    "        \n",
    "        #create folder\n",
    "        foldername = datatarget + str(label)\n",
    "        \n",
    "        if not path.exists(foldername):\n",
    "            os.mkdir(foldername)\n",
    "        \n",
    "        #move each file\n",
    "        for file in list(df[label]):\n",
    "            \n",
    "            fileoriginal =  datafolder + file + \".jpg\"\n",
    "            filetarget = datatarget +\"/\" + file + \".jpg\"\n",
    "            \n",
    "            try:\n",
    "                if not path.exists(filetarget):\n",
    "                    shutil.copyfile(fileoriginal, filetarget)\n",
    "\n",
    "            except FileNotFoundError:\n",
    "                print(\"File {} not found!\".format(file))\n",
    "                pass\n",
    "\n",
    "#origin folder\n",
    "imagefolder = \"../data/food-101/images/\"\n",
    "\n",
    "\n",
    "organise_files_from_df(train_meta, imagefolder, trainfolder)\n",
    "\n",
    "organise_files_from_df(valid_meta, imagefolder, validfolder)\n",
    "\n",
    "organise_files_from_df(test_meta, imagefolder, testfolder)\n",
    "\n",
    "#to delete the origin folder, uncomment the line below\n",
    "#shutil.rmtree(imagefolder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data to S3\n",
    "\n",
    ">The below cells load in some AWS SageMaker libraries, starts a SageMaker session and creates a default bucket. After creating this bucket, it upload the locally stored data to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "# session and role\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# create an S3 bucket\n",
    "bucket = sagemaker_session.default_bucket()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Upload training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data uploaded to s3 after 293.68911957740784 seconds\n"
     ]
    }
   ],
   "source": [
    "prefix = \"food-classifier\"\n",
    "datafolder = \"../data/s3_train_data\"\n",
    "# upload all data to S3\n",
    "\n",
    "#this is slow!\n",
    "start = time.time()\n",
    "input_data = sagemaker_session.upload_data(path=datafolder, bucket=bucket, key_prefix=prefix)\n",
    "end = time.time()\n",
    "\n",
    "print(\"Data uploaded to s3 after {} seconds\".format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All good!\n"
     ]
    }
   ],
   "source": [
    "# check that data is in S3 bucket\n",
    "empty_check = []\n",
    "for obj in boto3.resource('s3').Bucket(bucket).objects.all():\n",
    "    empty_check.append(obj.key)\n",
    "    #print(obj.key)\n",
    "\n",
    "assert len(empty_check) !=0, 'S3 bucket is empty.'\n",
    "print('All good!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Checking model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorchvision.models\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mmodels\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m#importing pretrained ResNet for transfer learning\u001b[39;49;00m\n",
      "ResNetTransfer = models.resnet50(pretrained=\u001b[36mTrue\u001b[39;49;00m) \u001b[37m#.load_state_dict(torch.load(\"resnet50_base.pt\"))\u001b[39;49;00m\n"
     ]
    }
   ],
   "source": [
    "!pygmentize pytorch_source/model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Checking train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch.optim\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36moptim\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch.nn\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnn\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch.utils.data\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtorchvision\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m datasets\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorchvision.transforms\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mtransforms\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch.optim\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36moptim\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtorch.optim.lr_scheduler\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m ReduceLROnPlateau\n",
      "\n",
      "\u001b[37m# imports the model in model.py by name\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mmodel\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m ResNetTransfer\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmodel_fn\u001b[39;49;00m(model_dir):\n",
      "    \u001b[33m\"\"\"Load the PyTorch model from the `model_dir` directory.\"\"\"\u001b[39;49;00m\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mLoading model.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[37m# First, load the parameters used to create the model.\u001b[39;49;00m\n",
      "    model_info = {}\n",
      "    model_info_path = os.path.join(model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel_info.pth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(model_info_path, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\n",
      "        model_info = torch.load(f)\n",
      "\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mmodel_info: {}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(model_info))\n",
      "\n",
      "    \u001b[37m# Determine the device and construct the model.\u001b[39;49;00m\n",
      "    device = torch.device(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m torch.cuda.is_available() \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mUsing device {}.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(device))\n",
      "\n",
      "    model = ResNetTransfer\n",
      "\n",
      "    \u001b[37m#freezing the parameters\u001b[39;49;00m\n",
      "    \u001b[34mfor\u001b[39;49;00m param \u001b[35min\u001b[39;49;00m model.parameters():\n",
      "        param.requires_grad = \u001b[36mFalse\u001b[39;49;00m\n",
      "    model.fc = nn.Linear(model.fc.in_features, model_info[\u001b[33m\"\u001b[39;49;00m\u001b[33mn_classes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "\n",
      "    \u001b[37m# Load the stored model parameters.\u001b[39;49;00m\n",
      "    model_path = os.path.join(model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel.pth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(model_path, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\n",
      "        model.load_state_dict(torch.load(f))\n",
      "\n",
      "    \u001b[37m# set to eval mode, could use no_grad\u001b[39;49;00m\n",
      "    model.to(device).eval()\n",
      "\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mDone loading model.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \u001b[34mreturn\u001b[39;49;00m model\n",
      "\n",
      "\n",
      "\u001b[37m# Gets prepared training data for the Dataloaders\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_get_train_data_loader\u001b[39;49;00m(img_short_side_resize, img_input_size, norm_mean, norm_std,\n",
      "                           shuffle, num_workers, batch_size, datadir, trainfolder, validfolder):\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mGet train data loader.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "    transform_train = transforms.Compose([\n",
      "                    transforms.Resize(img_short_side_resize),\n",
      "                    transforms.ColorJitter(brightness=\u001b[34m0.2\u001b[39;49;00m, contrast=\u001b[34m0.2\u001b[39;49;00m, saturation=\u001b[34m0.2\u001b[39;49;00m, hue=\u001b[34m0.1\u001b[39;49;00m),\n",
      "                    transforms.RandomHorizontalFlip(),\n",
      "                    transforms.RandomResizedCrop(img_input_size, scale=(\u001b[34m0.08\u001b[39;49;00m,\u001b[34m1\u001b[39;49;00m), ratio=(\u001b[34m1\u001b[39;49;00m,\u001b[34m1\u001b[39;49;00m)), \n",
      "                    transforms.ToTensor(),\n",
      "                    transforms.Normalize(mean = norm_mean, std = norm_std)])\n",
      "    transform_test = transforms.Compose([\n",
      "                        transforms.Resize(img_input_size),  \n",
      "                        transforms.FiveCrop(img_input_size),\n",
      "                        transforms.Lambda(\u001b[34mlambda\u001b[39;49;00m crops: torch.stack([transforms.Compose([\n",
      "                        transforms.ToTensor(),\n",
      "                        transforms.Normalize(mean = norm_mean, std = norm_std)])(crop) \u001b[34mfor\u001b[39;49;00m crop \u001b[35min\u001b[39;49;00m crops]))])\n",
      "\n",
      "    train_data = datasets.ImageFolder(datadir + trainfolder, transform_train)\n",
      "    valid_data = datasets.ImageFolder(datadir + validfolder, transform_test)\n",
      "\n",
      "    \u001b[37m# Create the data loaders\u001b[39;49;00m\n",
      "    data = {\u001b[33m\"\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m : train_data, \u001b[33m\"\u001b[39;49;00m\u001b[33mval\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:valid_data}\n",
      "\n",
      "    train_loader = torch.utils.data.DataLoader(data[\u001b[33m\"\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], batch_size=batch_size, num_workers=num_workers, shuffle=shuffle, pin_memory=\u001b[36mTrue\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[37m#### --- NOTE on num_workers if using 5crop and batch_size for testing --- ###\u001b[39;49;00m\n",
      "    \u001b[37m# If using the 5crop test time augmentation, num_workers = 0 (an error is raised otherwise) \u001b[39;49;00m\n",
      "    \u001b[37m# batch_size needs to be reduced during testing due to memory requirements\u001b[39;49;00m\n",
      "    valid_loader = torch.utils.data.DataLoader(data[\u001b[33m\"\u001b[39;49;00m\u001b[33mval\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], batch_size=\u001b[36mint\u001b[39;49;00m(np.floor(batch_size/\u001b[34m5\u001b[39;49;00m)), num_workers=\u001b[34m0\u001b[39;49;00m, shuffle=shuffle, pin_memory=\u001b[36mTrue\u001b[39;49;00m)\n",
      "\n",
      "    loaders_transfer = {\u001b[33m\"\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m : train_loader, \u001b[33m\"\u001b[39;49;00m\u001b[33mval\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:valid_loader}\n",
      "\n",
      "    \u001b[34mreturn\u001b[39;49;00m loaders_transfer\n",
      "\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtime\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mdatetime\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtrain_epoch\u001b[39;49;00m(model,train_loader,optimizer,criterion,device):\n",
      "    \u001b[33m\"\"\"\u001b[39;49;00m\n",
      "\u001b[33m    train steps at each epoch\u001b[39;49;00m\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\n",
      "    train_loss = \u001b[34m0.0\u001b[39;49;00m\n",
      "    \n",
      "    model.train()\n",
      "    \n",
      "    \u001b[34mfor\u001b[39;49;00m batch_idx, (data, target) \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(train_loader):\n",
      "        \n",
      "        data, target = data.to(device), target.to(device) \u001b[37m# move to GPU\u001b[39;49;00m\n",
      "        \n",
      "        optimizer.zero_grad() \u001b[37m# set gradients to 0\u001b[39;49;00m\n",
      "        \n",
      "        output = model(data) \u001b[37m# get output\u001b[39;49;00m\n",
      "        \n",
      "        loss = criterion(output, target) \u001b[37m# calculate loss\u001b[39;49;00m\n",
      "        train_loss += loss.item() * data.size(\u001b[34m0\u001b[39;49;00m)\n",
      "        \n",
      "        loss.backward() \u001b[37m# calculate gradients\u001b[39;49;00m\n",
      "        \n",
      "        optimizer.step() \u001b[37m# take step\u001b[39;49;00m\n",
      "        \n",
      "    train_loss = train_loss / \u001b[36mlen\u001b[39;49;00m(train_loader.dataset)\n",
      "    \u001b[34mreturn\u001b[39;49;00m model, train_loss\n",
      "        \n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mvalid_epoch\u001b[39;49;00m(model, valid_loader, criterion, device, fivecrop):\n",
      "    \u001b[33m\"\"\"\u001b[39;49;00m\n",
      "\u001b[33m    validation prediction steps at each epoch\u001b[39;49;00m\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\n",
      "    valid_loss = \u001b[34m0.0\u001b[39;49;00m\n",
      "    \n",
      "    model.eval()\n",
      "    \n",
      "    \u001b[34mwith\u001b[39;49;00m torch.no_grad():\n",
      "        \u001b[34mfor\u001b[39;49;00m data, target \u001b[35min\u001b[39;49;00m valid_loader:\n",
      "            \n",
      "            data, target = data.to(device), target.to(device) \u001b[37m# move to GPU\u001b[39;49;00m\n",
      "            \n",
      "            \u001b[37m# if we do test time augmentation with 5crop we'll have an extra dimension in our tensor\u001b[39;49;00m\n",
      "            \u001b[34mif\u001b[39;49;00m fivecrop == \u001b[33m\"\u001b[39;49;00m\u001b[33mmean\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\n",
      "                bs, ncrops, c, h, w = data.size()\n",
      "                output = model(data.view(-\u001b[34m1\u001b[39;49;00m, c, h, w)) \u001b[37m# fuse batch size and ncrops\u001b[39;49;00m\n",
      "                output = output.view(bs, ncrops, -\u001b[34m1\u001b[39;49;00m).mean(\u001b[34m1\u001b[39;49;00m)\n",
      "            \u001b[34melif\u001b[39;49;00m fivecrop == \u001b[33m\"\u001b[39;49;00m\u001b[33mmax\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\n",
      "                bs, ncrops, c, h, w = data.size()\n",
      "                output = model(data.view(-\u001b[34m1\u001b[39;49;00m, c, h, w)) \u001b[37m# fuse batch size and ncrops\u001b[39;49;00m\n",
      "                output = output.view(bs, ncrops, -\u001b[34m1\u001b[39;49;00m).max(\u001b[34m1\u001b[39;49;00m)[\u001b[34m0\u001b[39;49;00m]\n",
      "            \u001b[34melse\u001b[39;49;00m:\n",
      "                output = model(data)\n",
      "                \n",
      "            \u001b[37m## update the average validation loss\u001b[39;49;00m\n",
      "            loss = criterion(output, target)\n",
      "            valid_loss += loss.item() * data.size(\u001b[34m0\u001b[39;49;00m)\n",
      "            \n",
      "    valid_loss = valid_loss / \u001b[36mlen\u001b[39;49;00m(valid_loader.dataset) \n",
      "    \u001b[34mreturn\u001b[39;49;00m valid_loss\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtrain\u001b[39;49;00m(n_epochs, loaders, model, optimizer, criterion, device, path_model, fivecrop = \u001b[36mNone\u001b[39;49;00m, lr_scheduler = \u001b[36mNone\u001b[39;49;00m):\n",
      "    \u001b[33m\"\"\"\u001b[39;49;00m\n",
      "\u001b[33m    model training\u001b[39;49;00m\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\n",
      "    \n",
      "    \u001b[37m# initialize tracker for minimum validation loss\u001b[39;49;00m\n",
      "    valid_loss_min = np.Inf \n",
      "    train_loss = []\n",
      "    valid_loss = []\n",
      "    \n",
      "    time_start = time.time()\n",
      "    best_epoch = \u001b[34m0\u001b[39;49;00m\n",
      "    \n",
      "    \u001b[34mfor\u001b[39;49;00m epoch \u001b[35min\u001b[39;49;00m \u001b[36mrange\u001b[39;49;00m(\u001b[34m1\u001b[39;49;00m, n_epochs+\u001b[34m1\u001b[39;49;00m):\n",
      "        \n",
      "        time_start_epoch = time.time()  \n",
      "        \n",
      "        \u001b[37m# train current epoch\u001b[39;49;00m\n",
      "        model, train_loss_epoch = train_epoch(model, loaders[\u001b[33m\"\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], optimizer, criterion, device) \n",
      "        train_loss.append(train_loss_epoch)   \n",
      "        \n",
      "        \u001b[37m# validate current epoch\u001b[39;49;00m\n",
      "        valid_loss_epoch = valid_epoch(model,loaders[\u001b[33m\"\u001b[39;49;00m\u001b[33mval\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m],criterion,device,fivecrop)\n",
      "        \n",
      "        \u001b[37m# learning rate scheduler\u001b[39;49;00m\n",
      "        \u001b[34mif\u001b[39;49;00m lr_scheduler \u001b[35mis\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m \u001b[36mNone\u001b[39;49;00m:\n",
      "            lr_scheduler.step(valid_loss_epoch)\n",
      "        valid_loss.append(valid_loss_epoch)  \n",
      "        \n",
      "        \u001b[34mif\u001b[39;49;00m valid_loss_epoch <= valid_loss_min: \u001b[37m# save if validation loss is the lowest so far\u001b[39;49;00m\n",
      "            torch.save(model.state_dict(), path_model)\n",
      "            valid_loss_min = valid_loss_epoch \n",
      "            best_epoch = epoch\n",
      "            \n",
      "        \u001b[37m# print epoch stats\u001b[39;49;00m\n",
      "        currentDT = datetime.datetime.now()\n",
      "        exact_time =  \u001b[36mstr\u001b[39;49;00m(currentDT.hour) + \u001b[33m\"\u001b[39;49;00m\u001b[33m:\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m + \u001b[36mstr\u001b[39;49;00m(currentDT.minute) + \u001b[33m\"\u001b[39;49;00m\u001b[33m:\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m + \u001b[36mstr\u001b[39;49;00m(currentDT.second)\n",
      "        \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mEpoch {} done in {:.2f} seconds at {}. \u001b[39;49;00m\u001b[33m\\t\u001b[39;49;00m\u001b[33mTraining Loss: {:.3f} \u001b[39;49;00m\u001b[33m\\t\u001b[39;49;00m\u001b[33mValidation Loss: {:.3f}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format( \n",
      "            epoch,             \n",
      "            time.time() - time_start_epoch,\n",
      "            exact_time,\n",
      "            train_loss_epoch,\n",
      "            valid_loss_epoch\n",
      "            ))   \n",
      "        \n",
      "    \u001b[37m# print final statistics    \u001b[39;49;00m\n",
      "    \u001b[34mprint\u001b[39;49;00m(f\u001b[33m\"\u001b[39;49;00m\u001b[33m{n_epochs} epochs trained in {(time.time() - time_start):.3f} seconds. \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \n",
      "    \n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mBest model obtained at epoch {} with minimum validation loss : {:.3f}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(best_epoch, valid_loss_min))\n",
      "    \n",
      "    \u001b[37m# Load best config\u001b[39;49;00m\n",
      "    model.load_state_dict(torch.load(path_model))\n",
      "    \n",
      "    \u001b[34mreturn\u001b[39;49;00m model\n",
      "\n",
      "\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m'\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\n",
      "    \n",
      "    \u001b[37m# Parameters settings\u001b[39;49;00m\n",
      "    parser = argparse.ArgumentParser()\n",
      "\n",
      "    \u001b[37m# SageMaker parameters\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--output-data-dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_OUTPUT_DATA_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--model-dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--data-dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CHANNEL_TRAINING\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    \n",
      "    \u001b[37m#Data preparation parameters\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--img_short_side_resize\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m256\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mI\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mResize to (default: 256)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--img_input_size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m224\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mI\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mImage input to ResNet (default: 224)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--shuffle\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mbool\u001b[39;49;00m, default=\u001b[36mTrue\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mI\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mDataloader shuffle (default: True)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--num_workers\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m16\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mI\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mnumber of workers in data preparation (default: 16)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--trainfolder\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m'\u001b[39;49;00m\u001b[33m/train_img\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mI\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mName of the folder containing training data (default: train_img)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--validfolder\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m'\u001b[39;49;00m\u001b[33m/valid_img\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mI\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mName of the folder containing validation data (default: valid_img)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[37m# Training Parameters\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--batch_size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m64\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33minput batch size for training (default: 64)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--n_epochs\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m10\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mnumber of epochs to train (default: 10)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--seed\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m42\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mS\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mrandom seed (default: 42)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--lr\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m3e-4\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mLR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mlearning rate (default: 3e-4)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \n",
      "    \u001b[37m# Model Parameters\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--n_classes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m2\u001b[39;49;00m, metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mM\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33moutput dimension of the model (int - default: 2)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \n",
      "    \n",
      "    \u001b[37m# args holds all passed-in arguments\u001b[39;49;00m\n",
      "    args = parser.parse_args()\n",
      "\n",
      "    device = torch.device(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m torch.cuda.is_available() \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mUsing device {}.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(device))\n",
      "\n",
      "    torch.manual_seed(args.seed)\n",
      "\n",
      "    \u001b[37m# Load the training data.\u001b[39;49;00m\n",
      "    \u001b[37m#setting norm_mean and norm_std\u001b[39;49;00m\n",
      "    norm_mean = [\u001b[34m0.485\u001b[39;49;00m, \u001b[34m0.456\u001b[39;49;00m, \u001b[34m0.406\u001b[39;49;00m]\n",
      "    norm_std = [\u001b[34m0.229\u001b[39;49;00m, \u001b[34m0.224\u001b[39;49;00m, \u001b[34m0.225\u001b[39;49;00m]\n",
      "\n",
      "    train_loader = _get_train_data_loader(args.img_short_side_resize, \n",
      "                                          args.img_input_size,\n",
      "                                          norm_mean,\n",
      "                                          norm_std,\n",
      "                                          args.shuffle,\n",
      "                                          args.num_workers,\n",
      "                                          args.batch_size, \n",
      "                                          args.data_dir,\n",
      "                                          args.trainfolder,\n",
      "                                          args.validfolder)\n",
      "\n",
      "    \u001b[37m# Load the ResNet model\u001b[39;49;00m\n",
      "    model = ResNetTransfer\n",
      "\n",
      "    \u001b[37m#freezing the parameters\u001b[39;49;00m\n",
      "    \u001b[34mfor\u001b[39;49;00m param \u001b[35min\u001b[39;49;00m model.parameters():\n",
      "        param.requires_grad = \u001b[36mFalse\u001b[39;49;00m\n",
      "        \n",
      "    \u001b[37m# Replacing the last layer with a fully connected layer to retrain\u001b[39;49;00m\n",
      "    model.fc = nn.Linear(model.fc.in_features, args.n_classes) \n",
      "\n",
      "    \u001b[37m# Initialize the weights of the new layer\u001b[39;49;00m\n",
      "    nn.init.kaiming_normal_(model.fc.weight, nonlinearity=\u001b[33m'\u001b[39;49;00m\u001b[33mrelu\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[37m# Transfer to GPU \u001b[39;49;00m\n",
      "    model = model.to(device)\n",
      "\n",
      "    \u001b[37m## Optimizer and loss function for training\u001b[39;49;00m\n",
      "    criterion_transfer = nn.CrossEntropyLoss()\n",
      "    optimizer_transfer = optim.Adam(model.parameters(),args.lr) \n",
      "    scheduler_transfer = ReduceLROnPlateau(optimizer_transfer, \u001b[33m'\u001b[39;49;00m\u001b[33mmin\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, verbose = \u001b[36mTrue\u001b[39;49;00m, factor = \u001b[34m0.5\u001b[39;49;00m, patience = \u001b[34m7\u001b[39;49;00m)\n",
      "    \n",
      "\n",
      "    \u001b[37m# Trains the model \u001b[39;49;00m\n",
      "    train(args.n_epochs, \n",
      "          train_loader, \n",
      "          model, \n",
      "          optimizer_transfer, \n",
      "          criterion_transfer, \n",
      "          device, \n",
      "          os.path.join(args.model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel.pth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m), \n",
      "          fivecrop = \u001b[33m\"\u001b[39;49;00m\u001b[33mmean\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \n",
      "          lr_scheduler = scheduler_transfer)\n",
      "\n",
      "    \u001b[37m# Keep the keys of this dictionary as they are \u001b[39;49;00m\n",
      "    model_info_path = os.path.join(args.model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel_info.pth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(model_info_path, \u001b[33m'\u001b[39;49;00m\u001b[33mwb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\n",
      "        model_info = {\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33mn_classes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: args.n_classes,\n",
      "        }\n",
      "        torch.save(model_info, f)\n",
      "        \n",
      "    \u001b[37m# Save the model parameters\u001b[39;49;00m\n",
      "    model_path = os.path.join(args.model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel.pth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(model_path, \u001b[33m'\u001b[39;49;00m\u001b[33mwb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\n",
      "        torch.save(model.state_dict(), f)\n"
     ]
    }
   ],
   "source": [
    "!pygmentize pytorch_source/train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Create pytorch estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import a PyTorch wrapper\n",
    "from sagemaker.pytorch import PyTorch, PyTorchModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify an output path\n",
    "# prefix is specified above\n",
    "output_path = 's3://{}/{}'.format(bucket, prefix)\n",
    "\n",
    "# instantiate a pytorch estimator\n",
    "estimator = PyTorch(entry_point='train.py',\n",
    "                    source_dir='pytorch_source', \n",
    "                    role=role,\n",
    "                    framework_version= '1.1.0', #'1.3.1',\n",
    "                    train_instance_count=1,\n",
    "                    train_instance_type='ml.p2.xlarge',\n",
    "                    output_path=output_path,\n",
    "                    sagemaker_session=sagemaker_session,\n",
    "                    hyperparameters={\n",
    "                        'n_classes': nc + 1,  # num of classes for the fully connected layer at the end of the network (defined on the first cells)\n",
    "                        'n_epochs': 5,\n",
    "                        'img_short_side_resize':256,\n",
    "                        'img_input_size':224,\n",
    "                        'num_workers':16,\n",
    "                        'batch_size':64\n",
    "                    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-03 15:54:05 Starting - Starting the training job...\n",
      "2020-03-03 15:54:07 Starting - Launching requested ML instances......\n",
      "2020-03-03 15:55:05 Starting - Insufficient capacity error from EC2 while launching instances, retrying!......\n",
      "2020-03-03 15:56:07 Starting - Preparing the instances for training......\n",
      "2020-03-03 15:57:31 Downloading - Downloading input data......\n",
      "2020-03-03 15:58:25 Training - Downloading the training image..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2020-03-03 15:58:46,303 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2020-03-03 15:58:46,330 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2020-03-03 15:58:46,959 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2020-03-03 15:58:48,904 sagemaker-containers INFO     Module train does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2020-03-03 15:58:48,904 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2020-03-03 15:58:48,904 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2020-03-03 15:58:48,904 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/usr/bin/python -m pip install . \u001b[0m\n",
      "\u001b[34mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: train\n",
      "  Running setup.py bdist_wheel for train: started\u001b[0m\n",
      "\n",
      "2020-03-03 15:59:05 Training - Training image download completed. Training in progress.\u001b[34m  Running setup.py bdist_wheel for train: finished with status 'done'\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-yg6a0h3e/wheels/35/24/16/37574d11bf9bde50616c67372a334f94fa8356bc7164af8ca3\u001b[0m\n",
      "\u001b[34mSuccessfully built train\u001b[0m\n",
      "\u001b[34mInstalling collected packages: train\u001b[0m\n",
      "\u001b[34mSuccessfully installed train-1.0.0\u001b[0m\n",
      "\u001b[34mYou are using pip version 18.1, however version 20.0.2 is available.\u001b[0m\n",
      "\u001b[34mYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[34m2020-03-03 15:58:57,427 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch_size\": 64,\n",
      "        \"n_epochs\": 5,\n",
      "        \"num_workers\": 16,\n",
      "        \"img_input_size\": 224,\n",
      "        \"n_classes\": 6,\n",
      "        \"img_short_side_resize\": 256\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"sagemaker-pytorch-2020-03-03-15-53-46-306\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-eu-central-1-515611759963/sagemaker-pytorch-2020-03-03-15-53-46-306/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"batch_size\":64,\"img_input_size\":224,\"img_short_side_resize\":256,\"n_classes\":6,\"n_epochs\":5,\"num_workers\":16}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-eu-central-1-515611759963/sagemaker-pytorch-2020-03-03-15-53-46-306/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch_size\":64,\"img_input_size\":224,\"img_short_side_resize\":256,\"n_classes\":6,\"n_epochs\":5,\"num_workers\":16},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"sagemaker-pytorch-2020-03-03-15-53-46-306\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-eu-central-1-515611759963/sagemaker-pytorch-2020-03-03-15-53-46-306/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--batch_size\",\"64\",\"--img_input_size\",\"224\",\"--img_short_side_resize\",\"256\",\"--n_classes\",\"6\",\"--n_epochs\",\"5\",\"--num_workers\",\"16\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH_SIZE=64\u001b[0m\n",
      "\u001b[34mSM_HP_N_EPOCHS=5\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_WORKERS=16\u001b[0m\n",
      "\u001b[34mSM_HP_IMG_INPUT_SIZE=224\u001b[0m\n",
      "\u001b[34mSM_HP_N_CLASSES=6\u001b[0m\n",
      "\u001b[34mSM_HP_IMG_SHORT_SIDE_RESIZE=256\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/usr/local/bin:/usr/lib/python36.zip:/usr/lib/python3.6:/usr/lib/python3.6/lib-dynload:/usr/local/lib/python3.6/dist-packages:/usr/lib/python3/dist-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/usr/bin/python -m train --batch_size 64 --img_input_size 224 --img_short_side_resize 256 --n_classes 6 --n_epochs 5 --num_workers 16\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mUsing device cuda.\u001b[0m\n",
      "\u001b[34mGet train data loader.\u001b[0m\n",
      "\u001b[34mEpoch 1 done in 68.59 seconds at 16:0:20. #011Training Loss: 1.466 #011Validation Loss: 1.063\u001b[0m\n",
      "\u001b[34mEpoch 2 done in 68.38 seconds at 16:1:28. #011Training Loss: 1.033 #011Validation Loss: 0.762\u001b[0m\n",
      "\u001b[34mEpoch 3 done in 68.10 seconds at 16:2:36. #011Training Loss: 0.848 #011Validation Loss: 0.631\u001b[0m\n",
      "\u001b[34mEpoch 4 done in 67.66 seconds at 16:3:44. #011Training Loss: 0.742 #011Validation Loss: 0.544\u001b[0m\n",
      "\u001b[34mEpoch 5 done in 67.69 seconds at 16:4:52. #011Training Loss: 0.682 #011Validation Loss: 0.479\u001b[0m\n",
      "\u001b[34m5 epochs trained in 340.420 seconds. \u001b[0m\n",
      "\u001b[34mBest model obtained at epoch 5 with minimum validation loss : 0.479\u001b[0m\n",
      "\u001b[34m2020-03-03 16:04:53,326 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2020-03-03 16:04:57 Uploading - Uploading generated training model\n",
      "2020-03-03 16:05:14 Completed - Training job completed\n",
      "Training seconds: 463\n",
      "Billable seconds: 463\n"
     ]
    }
   ],
   "source": [
    "estimator.fit({'training': input_data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Deploy the model for testing\n",
    "\n",
    "The model will be tested by first deploying it and then sending the testing data to the deployed endpoint\n",
    "\n",
    "The function that loads the saved model is called `model_fn()` and takes as its only parameter a path to the directory where the model artifacts are stored. This function must also be present in the python file specified as the entry point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In case we want to retrieve an already trained model from sagemaker (data available into the 'model' section)\n",
    "\n",
    "estimator = sagemaker.model.FrameworkModel(\n",
    "    model_data='s3://sagemaker-eu-central-1-515611759963/food-classifier/sagemaker-pytorch-2020-03-03-15-53-46-306/output/model.tar.gz', #model location\n",
    "    image='520713654638.dkr.ecr.eu-central-1.amazonaws.com/sagemaker-pytorch:1.1.0-gpu-py3',  # image\n",
    "    role=role,\n",
    "    entry_point='train.py',\n",
    "    source_dir='pytorch_source',\n",
    "    sagemaker_session = sagemaker_session\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#estimator = PyTorchModel(model_data='s3://sagemaker-eu-central-1-515611759963/food-classifier/sagemaker-pytorch-2020-03-03-15-53-46-306/output/model.tar.gz', \n",
    "#                             role=role,\n",
    "#                             image='520713654638.dkr.ecr.eu-central-1.amazonaws.com/sagemaker-pytorch:1.1.0-gpu-py3',\n",
    "#                             entry_point='pytorch_source/train.py',\n",
    "#                            predictor_cls = 'pytorch_source/train.py',\n",
    "#                           framework_version = '1.1.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------!"
     ]
    }
   ],
   "source": [
    "predictor = estimator.deploy(instance_type='ml.p2.xlarge', initial_instance_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sagemaker.session.Session at 0x7fb1989da978>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator.sagemaker_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "ename": "ClientError",
     "evalue": "An error occurred (ValidationException) when calling the DescribeEndpoint operation: Could not find endpoint \"arn:aws:sagemaker:eu-central-1:515611759963:endpoint/sagemaker-pytorch-2020-03-03-15-53-46-306\".",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-91-b1c1225bf982>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m predictor = sagemaker.predictor.RealTimePredictor(\n\u001b[1;32m      2\u001b[0m     \u001b[0mendpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sagemaker-pytorch-2020-03-03-15-53-46-306'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     sagemaker_session = sagemaker_session)\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;31m#content_type='image/jpeg',\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m#accept='image/png')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/predictor.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, endpoint, sagemaker_session, serializer, deserializer, content_type, accept)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontent_type\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mserializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"content_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccept\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccept\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeserializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"accept\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_endpoint_config_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_endpoint_config_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_model_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/predictor.py\u001b[0m in \u001b[0;36m_get_endpoint_config_name\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0;34m\"\"\"Placeholder docstring\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         endpoint_desc = self.sagemaker_session.sagemaker_client.describe_endpoint(\n\u001b[0;32m--> 280\u001b[0;31m             \u001b[0mEndpointName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m         )\n\u001b[1;32m    282\u001b[0m         \u001b[0mendpoint_config_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mendpoint_desc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"EndpointConfigName\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    274\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[1;32m    275\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    584\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mClientError\u001b[0m: An error occurred (ValidationException) when calling the DescribeEndpoint operation: Could not find endpoint \"arn:aws:sagemaker:eu-central-1:515611759963:endpoint/sagemaker-pytorch-2020-03-03-15-53-46-306\"."
     ]
    }
   ],
   "source": [
    "predictor = sagemaker.predictor.RealTimePredictor(\n",
    "    endpoint='sagemaker-pytorch-2020-03-03-15-53-46-306',\n",
    "    sagemaker_session = sagemaker_session)\n",
    "    #content_type='image/jpeg',\n",
    "    #accept='image/png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictor = estimator.deploy(initial_instance_count=1, instance_type=\"ml.p2.xlarge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method RealTimePredictor.predict of <sagemaker.predictor.RealTimePredictor object at 0x7fb196242cc0>>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Transformers set-up for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Norm values\n",
    "norm_mean = [0.485, 0.456, 0.406]\n",
    "norm_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "#Img size parameters\n",
    "img_short_side_resize = 256\n",
    "img_input_size = 224\n",
    "\n",
    "import os\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "                    transforms.Resize(img_input_size),  \n",
    "                    transforms.FiveCrop(img_input_size),\n",
    "                    transforms.Lambda(lambda crops: torch.stack([transforms.Compose([\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean = norm_mean, std = norm_std)])(crop) for crop in crops]))])\n",
    "\n",
    "test_data = datasets.ImageFolder(testfolder, transform_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardTransform\n",
       "Transform: Compose(\n",
       "               Resize(size=224, interpolation=PIL.Image.BILINEAR)\n",
       "               FiveCrop(size=(224, 224))\n",
       "               Lambda()\n",
       "           )"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParamValidationError",
     "evalue": "Parameter validation failed:\nInvalid type for parameter Body, value: tensor([[[1.1872, 1.3070, 1.3242,  ..., 1.6153, 1.5982, 1.5639],\n         [1.2385, 1.3242, 1.3413,  ..., 1.6153, 1.5639, 1.5639],\n         [1.2899, 1.3413, 1.3584,  ..., 1.6495, 1.5982, 1.5810],\n         ...,\n         [1.1700, 1.1872, 1.2214,  ..., 1.1187, 1.1015, 1.1529],\n         [1.2385, 1.2214, 1.2385,  ..., 1.0844, 1.0673, 1.1015],\n         [1.1872, 1.2043, 1.2385,  ..., 1.0331, 0.9988, 0.9817]],\n\n        [[1.3606, 1.4832, 1.5007,  ..., 2.0434, 2.0259, 2.0084],\n         [1.4132, 1.5007, 1.5182,  ..., 2.0434, 1.9909, 2.0259],\n         [1.4657, 1.5182, 1.5357,  ..., 2.0784, 2.0434, 2.0434],\n         ...,\n         [1.4832, 1.5007, 1.5357,  ..., 1.4307, 1.4307, 1.5007],\n         [1.5532, 1.5357, 1.5532,  ..., 1.4832, 1.5182, 1.5532],\n         [1.4832, 1.5007, 1.5182,  ..., 1.4657, 1.4832, 1.5007]],\n\n        [[1.4374, 1.5594, 1.6117,  ..., 2.1346, 2.0997, 2.0474],\n         [1.5071, 1.5942, 1.6291,  ..., 2.1520, 2.0823, 2.0823],\n         [1.5768, 1.6291, 1.6465,  ..., 2.2043, 2.1520, 2.1171],\n         ...,\n         [1.6814, 1.6988, 1.7337,  ..., 1.0539, 1.0365, 1.1062],\n         [1.7163, 1.7337, 1.7511,  ..., 1.4200, 1.4548, 1.5245],\n         [1.6465, 1.6640, 1.6988,  ..., 1.6117, 1.6291, 1.6814]]]), type: <class 'torch.Tensor'>, valid types: <class 'bytes'>, <class 'bytearray'>, file-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParamValidationError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-7382829f0847>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/predictor.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, data, initial_args, target_model)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mrequest_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_request_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_runtime_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mrequest_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    274\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[1;32m    275\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    557\u001b[0m         }\n\u001b[1;32m    558\u001b[0m         request_dict = self._convert_to_request_dict(\n\u001b[0;32m--> 559\u001b[0;31m             api_params, operation_model, context=request_context)\n\u001b[0m\u001b[1;32m    560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m         \u001b[0mservice_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_service_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mservice_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyphenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_convert_to_request_dict\u001b[0;34m(self, api_params, operation_model, context)\u001b[0m\n\u001b[1;32m    605\u001b[0m             api_params, operation_model, context)\n\u001b[1;32m    606\u001b[0m         request_dict = self._serializer.serialize_to_request(\n\u001b[0;32m--> 607\u001b[0;31m             api_params, operation_model)\n\u001b[0m\u001b[1;32m    608\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_client_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minject_host_prefix\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m             \u001b[0mrequest_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'host_prefix'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/botocore/validate.py\u001b[0m in \u001b[0;36mserialize_to_request\u001b[0;34m(self, parameters, operation_model)\u001b[0m\n\u001b[1;32m    295\u001b[0m                                                     operation_model.input_shape)\n\u001b[1;32m    296\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mreport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_errors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mParamValidationError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreport\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m         return self._serializer.serialize_to_request(parameters,\n\u001b[1;32m    299\u001b[0m                                                      operation_model)\n",
      "\u001b[0;31mParamValidationError\u001b[0m: Parameter validation failed:\nInvalid type for parameter Body, value: tensor([[[1.1872, 1.3070, 1.3242,  ..., 1.6153, 1.5982, 1.5639],\n         [1.2385, 1.3242, 1.3413,  ..., 1.6153, 1.5639, 1.5639],\n         [1.2899, 1.3413, 1.3584,  ..., 1.6495, 1.5982, 1.5810],\n         ...,\n         [1.1700, 1.1872, 1.2214,  ..., 1.1187, 1.1015, 1.1529],\n         [1.2385, 1.2214, 1.2385,  ..., 1.0844, 1.0673, 1.1015],\n         [1.1872, 1.2043, 1.2385,  ..., 1.0331, 0.9988, 0.9817]],\n\n        [[1.3606, 1.4832, 1.5007,  ..., 2.0434, 2.0259, 2.0084],\n         [1.4132, 1.5007, 1.5182,  ..., 2.0434, 1.9909, 2.0259],\n         [1.4657, 1.5182, 1.5357,  ..., 2.0784, 2.0434, 2.0434],\n         ...,\n         [1.4832, 1.5007, 1.5357,  ..., 1.4307, 1.4307, 1.5007],\n         [1.5532, 1.5357, 1.5532,  ..., 1.4832, 1.5182, 1.5532],\n         [1.4832, 1.5007, 1.5182,  ..., 1.4657, 1.4832, 1.5007]],\n\n        [[1.4374, 1.5594, 1.6117,  ..., 2.1346, 2.0997, 2.0474],\n         [1.5071, 1.5942, 1.6291,  ..., 2.1520, 2.0823, 2.0823],\n         [1.5768, 1.6291, 1.6465,  ..., 2.2043, 2.1520, 2.1171],\n         ...,\n         [1.6814, 1.6988, 1.7337,  ..., 1.0539, 1.0365, 1.1062],\n         [1.7163, 1.7337, 1.7511,  ..., 1.4200, 1.4548, 1.5245],\n         [1.6465, 1.6640, 1.6988,  ..., 1.6117, 1.6291, 1.6814]]]), type: <class 'torch.Tensor'>, valid types: <class 'bytes'>, <class 'bytearray'>, file-like object"
     ]
    }
   ],
   "source": [
    "predictor.predict(test_data[0][0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "shuffle = True\n",
    "num_workers = 16\n",
    "batch_size = 64\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=int(np.floor(batch_size/5)), num_workers=0, shuffle=shuffle, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Testing function implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO BE IMPLEMENTED PROPERLY\n",
    "\n",
    "def test(loader, model, criterion, device):\n",
    "    \"\"\"\n",
    "    test function\n",
    "    \"\"\"\n",
    "    \n",
    "    test_loss = 0.\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "    \n",
    "    #model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(loader):\n",
    "            \n",
    "            data, target = data.to(device), target.to(device) # move to GPU\n",
    "            \n",
    "            bs, ncrops, c, h, w = data.size()\n",
    "            \n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model.predict(data.view(-1, c, h, w)) # fuse batch size and ncrops\n",
    "            output = output.view(bs, ncrops, -1).mean(1)    \n",
    "            \n",
    "            loss = criterion(output, target) # calculate the loss\n",
    "            \n",
    "            test_loss = test_loss + ((1 / (batch_idx + 1)) * (loss.data - test_loss)) # update average test loss \n",
    "            \n",
    "            pred = output.data.max(1, keepdim=True)[1] # convert output probabilities to predicted class\n",
    "            \n",
    "            # compare predictions to true label\n",
    "            correct += np.sum(np.squeeze(pred.eq(target.data.view_as(pred))).cpu().numpy()) \n",
    "            total += data.size(0)            \n",
    "            \n",
    "    print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "    print('\\nTest Accuracy: %2d%% (%2d/%2d)' % (\n",
    "        100. * correct / total, correct, total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cpu.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "print(\"Using device {}.\".format(device))\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Using\",torch.cuda.get_device_name(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictor.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParamValidationError",
     "evalue": "Parameter validation failed:\nInvalid type for parameter Body, value: tensor([[[[-1.9124, -1.8953, -1.8439,  ..., -2.0152, -2.0494, -2.0665],\n          [-1.8953, -1.8610, -1.8610,  ..., -2.0323, -2.0665, -2.0494],\n          [-1.8268, -1.8439, -1.8610,  ..., -2.0494, -2.0837, -2.0665],\n          ...,\n          [ 0.9988,  0.9988,  1.0159,  ...,  1.1358,  1.1529,  1.1700],\n          [ 0.9988,  1.0331,  1.0331,  ...,  1.1700,  1.2043,  1.2214],\n          [ 0.9817,  0.9988,  1.0502,  ...,  1.1872,  1.2385,  1.2728]],\n\n         [[-1.8431, -1.8256, -1.7731,  ..., -1.9307, -1.9657, -1.9832],\n          [-1.7906, -1.7731, -1.7556,  ..., -1.9482, -1.9832, -1.9657],\n          [-1.6856, -1.7031, -1.7206,  ..., -1.9657, -2.0007, -1.9832],\n          ...,\n          [ 1.0805,  1.0980,  1.0980,  ...,  1.2206,  1.2381,  1.2556],\n          [ 1.0630,  1.0980,  1.0980,  ...,  1.2556,  1.2906,  1.3081],\n          [ 1.0455,  1.0630,  1.1155,  ...,  1.2731,  1.3256,  1.3606]],\n\n         [[-1.6999, -1.6650, -1.6127,  ..., -1.6650, -1.6999, -1.7173],\n          [-1.6824, -1.6476, -1.6302,  ..., -1.6824, -1.7173, -1.6999],\n          [-1.6127, -1.6302, -1.6476,  ..., -1.6999, -1.7347, -1.7173],\n          ...,\n          [ 1.2980,  1.3154,  1.3154,  ...,  1.4200,  1.4374,  1.4548],\n          [ 1.3328,  1.3677,  1.3851,  ...,  1.4548,  1.4897,  1.5071],\n          [ 1.3502,  1.3851,  1.4200,  ...,  1.4722,  1.5245,  1.5594]]],\n\n\n        [[[-1.9124, -1.8953, -1.8439,  ..., -2.0152, -2.0494, -2.0665],\n          [-1.8953, -1.8610, -1.8610,  ..., -2.0323, -2.0665, -2.0494],\n          [-1.8268, -1.8439, -1.8610,  ..., -2.0494, -2.0837, -2.0665],\n          ...,\n          [ 0.9988,  0.9988,  1.0159,  ...,  1.1358,  1.1529,  1.1700],\n          [ 0.9988,  1.0331,  1.0331,  ...,  1.1700,  1.2043,  1.2214],\n          [ 0.9817,  0.9988,  1.0502,  ...,  1.1872,  1.2385,  1.2728]],\n\n         [[-1.8431, -1.8256, -1.7731,  ..., -1.9307, -1.9657, -1.9832],\n          [-1.7906, -1.7731, -1.7556,  ..., -1.9482, -1.9832, -1.9657],\n          [-1.6856, -1.7031, -1.7206,  ..., -1.9657, -2.0007, -1.9832],\n          ...,\n          [ 1.0805,  1.0980,  1.0980,  ...,  1.2206,  1.2381,  1.2556],\n          [ 1.0630,  1.0980,  1.0980,  ...,  1.2556,  1.2906,  1.3081],\n          [ 1.0455,  1.0630,  1.1155,  ...,  1.2731,  1.3256,  1.3606]],\n\n         [[-1.6999, -1.6650, -1.6127,  ..., -1.6650, -1.6999, -1.7173],\n          [-1.6824, -1.6476, -1.6302,  ..., -1.6824, -1.7173, -1.6999],\n          [-1.6127, -1.6302, -1.6476,  ..., -1.6999, -1.7347, -1.7173],\n          ...,\n          [ 1.2980,  1.3154,  1.3154,  ...,  1.4200,  1.4374,  1.4548],\n          [ 1.3328,  1.3677,  1.3851,  ...,  1.4548,  1.4897,  1.5071],\n          [ 1.3502,  1.3851,  1.4200,  ...,  1.4722,  1.5245,  1.5594]]],\n\n\n        [[[-1.9124, -1.8953, -1.8439,  ..., -2.0152, -2.0494, -2.0665],\n          [-1.8953, -1.8610, -1.8610,  ..., -2.0323, -2.0665, -2.0494],\n          [-1.8268, -1.8439, -1.8610,  ..., -2.0494, -2.0837, -2.0665],\n          ...,\n          [ 0.9988,  0.9988,  1.0159,  ...,  1.1358,  1.1529,  1.1700],\n          [ 0.9988,  1.0331,  1.0331,  ...,  1.1700,  1.2043,  1.2214],\n          [ 0.9817,  0.9988,  1.0502,  ...,  1.1872,  1.2385,  1.2728]],\n\n         [[-1.8431, -1.8256, -1.7731,  ..., -1.9307, -1.9657, -1.9832],\n          [-1.7906, -1.7731, -1.7556,  ..., -1.9482, -1.9832, -1.9657],\n          [-1.6856, -1.7031, -1.7206,  ..., -1.9657, -2.0007, -1.9832],\n          ...,\n          [ 1.0805,  1.0980,  1.0980,  ...,  1.2206,  1.2381,  1.2556],\n          [ 1.0630,  1.0980,  1.0980,  ...,  1.2556,  1.2906,  1.3081],\n          [ 1.0455,  1.0630,  1.1155,  ...,  1.2731,  1.3256,  1.3606]],\n\n         [[-1.6999, -1.6650, -1.6127,  ..., -1.6650, -1.6999, -1.7173],\n          [-1.6824, -1.6476, -1.6302,  ..., -1.6824, -1.7173, -1.6999],\n          [-1.6127, -1.6302, -1.6476,  ..., -1.6999, -1.7347, -1.7173],\n          ...,\n          [ 1.2980,  1.3154,  1.3154,  ...,  1.4200,  1.4374,  1.4548],\n          [ 1.3328,  1.3677,  1.3851,  ...,  1.4548,  1.4897,  1.5071],\n          [ 1.3502,  1.3851,  1.4200,  ...,  1.4722,  1.5245,  1.5594]]],\n\n\n        ...,\n\n\n        [[[-1.6384, -1.6042, -1.5870,  ..., -1.3815, -1.3815, -1.3987],\n          [-1.5357, -1.5014, -1.4843,  ..., -1.2617, -1.2617, -1.2788],\n          [-1.4500, -1.4329, -1.4329,  ..., -1.1589, -1.1760, -1.2103],\n          ...,\n          [-1.9124, -1.8953, -1.8782,  ..., -1.4500, -1.4672, -1.4843],\n          [-1.9124, -1.8953, -1.8953,  ..., -1.4672, -1.4843, -1.5014],\n          [-1.9124, -1.9124, -1.9124,  ..., -1.4843, -1.5014, -1.5014]],\n\n         [[-1.7381, -1.7381, -1.7381,  ..., -1.7906, -1.7906, -1.8256],\n          [-1.6856, -1.6681, -1.6681,  ..., -1.7206, -1.7206, -1.7381],\n          [-1.6331, -1.6331, -1.6506,  ..., -1.6856, -1.6856, -1.7031],\n          ...,\n          [-1.8256, -1.8081, -1.7906,  ..., -1.7556, -1.7731, -1.7731],\n          [-1.8256, -1.8081, -1.8081,  ..., -1.7731, -1.7731, -1.7731],\n          [-1.8256, -1.8256, -1.8256,  ..., -1.7731, -1.7731, -1.7731]],\n\n         [[-1.6476, -1.6302, -1.6127,  ..., -1.7870, -1.7870, -1.7870],\n          [-1.5779, -1.5604, -1.5604,  ..., -1.7522, -1.7522, -1.7522],\n          [-1.5430, -1.5430, -1.5604,  ..., -1.7522, -1.7522, -1.7696],\n          ...,\n          [-1.8044, -1.7870, -1.7696,  ..., -1.7696, -1.7870, -1.7870],\n          [-1.7870, -1.7696, -1.7870,  ..., -1.7870, -1.7870, -1.8044],\n          [-1.7696, -1.7696, -1.7696,  ..., -1.7870, -1.8044, -1.8044]]],\n\n\n        [[[-1.6384, -1.6042, -1.5870,  ..., -1.3815, -1.3815, -1.3987],\n          [-1.5357, -1.5014, -1.4843,  ..., -1.2617, -1.2617, -1.2788],\n          [-1.4500, -1.4329, -1.4329,  ..., -1.1589, -1.1760, -1.2103],\n          ...,\n          [-1.9124, -1.8953, -1.8782,  ..., -1.4500, -1.4672, -1.4843],\n          [-1.9124, -1.8953, -1.8953,  ..., -1.4672, -1.4843, -1.5014],\n          [-1.9124, -1.9124, -1.9124,  ..., -1.4843, -1.5014, -1.5014]],\n\n         [[-1.7381, -1.7381, -1.7381,  ..., -1.7906, -1.7906, -1.8256],\n          [-1.6856, -1.6681, -1.6681,  ..., -1.7206, -1.7206, -1.7381],\n          [-1.6331, -1.6331, -1.6506,  ..., -1.6856, -1.6856, -1.7031],\n          ...,\n          [-1.8256, -1.8081, -1.7906,  ..., -1.7556, -1.7731, -1.7731],\n          [-1.8256, -1.8081, -1.8081,  ..., -1.7731, -1.7731, -1.7731],\n          [-1.8256, -1.8256, -1.8256,  ..., -1.7731, -1.7731, -1.7731]],\n\n         [[-1.6476, -1.6302, -1.6127,  ..., -1.7870, -1.7870, -1.7870],\n          [-1.5779, -1.5604, -1.5604,  ..., -1.7522, -1.7522, -1.7522],\n          [-1.5430, -1.5430, -1.5604,  ..., -1.7522, -1.7522, -1.7696],\n          ...,\n          [-1.8044, -1.7870, -1.7696,  ..., -1.7696, -1.7870, -1.7870],\n          [-1.7870, -1.7696, -1.7870,  ..., -1.7870, -1.7870, -1.8044],\n          [-1.7696, -1.7696, -1.7696,  ..., -1.7870, -1.8044, -1.8044]]],\n\n\n        [[[-1.6384, -1.6042, -1.5870,  ..., -1.3815, -1.3815, -1.3987],\n          [-1.5357, -1.5014, -1.4843,  ..., -1.2617, -1.2617, -1.2788],\n          [-1.4500, -1.4329, -1.4329,  ..., -1.1589, -1.1760, -1.2103],\n          ...,\n          [-1.9124, -1.8953, -1.8782,  ..., -1.4500, -1.4672, -1.4843],\n          [-1.9124, -1.8953, -1.8953,  ..., -1.4672, -1.4843, -1.5014],\n          [-1.9124, -1.9124, -1.9124,  ..., -1.4843, -1.5014, -1.5014]],\n\n         [[-1.7381, -1.7381, -1.7381,  ..., -1.7906, -1.7906, -1.8256],\n          [-1.6856, -1.6681, -1.6681,  ..., -1.7206, -1.7206, -1.7381],\n          [-1.6331, -1.6331, -1.6506,  ..., -1.6856, -1.6856, -1.7031],\n          ...,\n          [-1.8256, -1.8081, -1.7906,  ..., -1.7556, -1.7731, -1.7731],\n          [-1.8256, -1.8081, -1.8081,  ..., -1.7731, -1.7731, -1.7731],\n          [-1.8256, -1.8256, -1.8256,  ..., -1.7731, -1.7731, -1.7731]],\n\n         [[-1.6476, -1.6302, -1.6127,  ..., -1.7870, -1.7870, -1.7870],\n          [-1.5779, -1.5604, -1.5604,  ..., -1.7522, -1.7522, -1.7522],\n          [-1.5430, -1.5430, -1.5604,  ..., -1.7522, -1.7522, -1.7696],\n          ...,\n          [-1.8044, -1.7870, -1.7696,  ..., -1.7696, -1.7870, -1.7870],\n          [-1.7870, -1.7696, -1.7870,  ..., -1.7870, -1.7870, -1.8044],\n          [-1.7696, -1.7696, -1.7696,  ..., -1.7870, -1.8044, -1.8044]]]]), type: <class 'torch.Tensor'>, valid types: <class 'bytes'>, <class 'bytearray'>, file-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParamValidationError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-5a71d135d521>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-70-61e6b3dca61f>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(loader, model, criterion, device)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;31m# forward pass: compute predicted outputs by passing inputs to the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# fuse batch size and ncrops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mncrops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/predictor.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, data, initial_args, target_model)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mrequest_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_request_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_runtime_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mrequest_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    274\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[1;32m    275\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    557\u001b[0m         }\n\u001b[1;32m    558\u001b[0m         request_dict = self._convert_to_request_dict(\n\u001b[0;32m--> 559\u001b[0;31m             api_params, operation_model, context=request_context)\n\u001b[0m\u001b[1;32m    560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m         \u001b[0mservice_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_service_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mservice_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyphenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_convert_to_request_dict\u001b[0;34m(self, api_params, operation_model, context)\u001b[0m\n\u001b[1;32m    605\u001b[0m             api_params, operation_model, context)\n\u001b[1;32m    606\u001b[0m         request_dict = self._serializer.serialize_to_request(\n\u001b[0;32m--> 607\u001b[0;31m             api_params, operation_model)\n\u001b[0m\u001b[1;32m    608\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_client_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minject_host_prefix\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m             \u001b[0mrequest_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'host_prefix'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/botocore/validate.py\u001b[0m in \u001b[0;36mserialize_to_request\u001b[0;34m(self, parameters, operation_model)\u001b[0m\n\u001b[1;32m    295\u001b[0m                                                     operation_model.input_shape)\n\u001b[1;32m    296\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mreport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_errors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mParamValidationError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreport\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m         return self._serializer.serialize_to_request(parameters,\n\u001b[1;32m    299\u001b[0m                                                      operation_model)\n",
      "\u001b[0;31mParamValidationError\u001b[0m: Parameter validation failed:\nInvalid type for parameter Body, value: tensor([[[[-1.9124, -1.8953, -1.8439,  ..., -2.0152, -2.0494, -2.0665],\n          [-1.8953, -1.8610, -1.8610,  ..., -2.0323, -2.0665, -2.0494],\n          [-1.8268, -1.8439, -1.8610,  ..., -2.0494, -2.0837, -2.0665],\n          ...,\n          [ 0.9988,  0.9988,  1.0159,  ...,  1.1358,  1.1529,  1.1700],\n          [ 0.9988,  1.0331,  1.0331,  ...,  1.1700,  1.2043,  1.2214],\n          [ 0.9817,  0.9988,  1.0502,  ...,  1.1872,  1.2385,  1.2728]],\n\n         [[-1.8431, -1.8256, -1.7731,  ..., -1.9307, -1.9657, -1.9832],\n          [-1.7906, -1.7731, -1.7556,  ..., -1.9482, -1.9832, -1.9657],\n          [-1.6856, -1.7031, -1.7206,  ..., -1.9657, -2.0007, -1.9832],\n          ...,\n          [ 1.0805,  1.0980,  1.0980,  ...,  1.2206,  1.2381,  1.2556],\n          [ 1.0630,  1.0980,  1.0980,  ...,  1.2556,  1.2906,  1.3081],\n          [ 1.0455,  1.0630,  1.1155,  ...,  1.2731,  1.3256,  1.3606]],\n\n         [[-1.6999, -1.6650, -1.6127,  ..., -1.6650, -1.6999, -1.7173],\n          [-1.6824, -1.6476, -1.6302,  ..., -1.6824, -1.7173, -1.6999],\n          [-1.6127, -1.6302, -1.6476,  ..., -1.6999, -1.7347, -1.7173],\n          ...,\n          [ 1.2980,  1.3154,  1.3154,  ...,  1.4200,  1.4374,  1.4548],\n          [ 1.3328,  1.3677,  1.3851,  ...,  1.4548,  1.4897,  1.5071],\n          [ 1.3502,  1.3851,  1.4200,  ...,  1.4722,  1.5245,  1.5594]]],\n\n\n        [[[-1.9124, -1.8953, -1.8439,  ..., -2.0152, -2.0494, -2.0665],\n          [-1.8953, -1.8610, -1.8610,  ..., -2.0323, -2.0665, -2.0494],\n          [-1.8268, -1.8439, -1.8610,  ..., -2.0494, -2.0837, -2.0665],\n          ...,\n          [ 0.9988,  0.9988,  1.0159,  ...,  1.1358,  1.1529,  1.1700],\n          [ 0.9988,  1.0331,  1.0331,  ...,  1.1700,  1.2043,  1.2214],\n          [ 0.9817,  0.9988,  1.0502,  ...,  1.1872,  1.2385,  1.2728]],\n\n         [[-1.8431, -1.8256, -1.7731,  ..., -1.9307, -1.9657, -1.9832],\n          [-1.7906, -1.7731, -1.7556,  ..., -1.9482, -1.9832, -1.9657],\n          [-1.6856, -1.7031, -1.7206,  ..., -1.9657, -2.0007, -1.9832],\n          ...,\n          [ 1.0805,  1.0980,  1.0980,  ...,  1.2206,  1.2381,  1.2556],\n          [ 1.0630,  1.0980,  1.0980,  ...,  1.2556,  1.2906,  1.3081],\n          [ 1.0455,  1.0630,  1.1155,  ...,  1.2731,  1.3256,  1.3606]],\n\n         [[-1.6999, -1.6650, -1.6127,  ..., -1.6650, -1.6999, -1.7173],\n          [-1.6824, -1.6476, -1.6302,  ..., -1.6824, -1.7173, -1.6999],\n          [-1.6127, -1.6302, -1.6476,  ..., -1.6999, -1.7347, -1.7173],\n          ...,\n          [ 1.2980,  1.3154,  1.3154,  ...,  1.4200,  1.4374,  1.4548],\n          [ 1.3328,  1.3677,  1.3851,  ...,  1.4548,  1.4897,  1.5071],\n          [ 1.3502,  1.3851,  1.4200,  ...,  1.4722,  1.5245,  1.5594]]],\n\n\n        [[[-1.9124, -1.8953, -1.8439,  ..., -2.0152, -2.0494, -2.0665],\n          [-1.8953, -1.8610, -1.8610,  ..., -2.0323, -2.0665, -2.0494],\n          [-1.8268, -1.8439, -1.8610,  ..., -2.0494, -2.0837, -2.0665],\n          ...,\n          [ 0.9988,  0.9988,  1.0159,  ...,  1.1358,  1.1529,  1.1700],\n          [ 0.9988,  1.0331,  1.0331,  ...,  1.1700,  1.2043,  1.2214],\n          [ 0.9817,  0.9988,  1.0502,  ...,  1.1872,  1.2385,  1.2728]],\n\n         [[-1.8431, -1.8256, -1.7731,  ..., -1.9307, -1.9657, -1.9832],\n          [-1.7906, -1.7731, -1.7556,  ..., -1.9482, -1.9832, -1.9657],\n          [-1.6856, -1.7031, -1.7206,  ..., -1.9657, -2.0007, -1.9832],\n          ...,\n          [ 1.0805,  1.0980,  1.0980,  ...,  1.2206,  1.2381,  1.2556],\n          [ 1.0630,  1.0980,  1.0980,  ...,  1.2556,  1.2906,  1.3081],\n          [ 1.0455,  1.0630,  1.1155,  ...,  1.2731,  1.3256,  1.3606]],\n\n         [[-1.6999, -1.6650, -1.6127,  ..., -1.6650, -1.6999, -1.7173],\n          [-1.6824, -1.6476, -1.6302,  ..., -1.6824, -1.7173, -1.6999],\n          [-1.6127, -1.6302, -1.6476,  ..., -1.6999, -1.7347, -1.7173],\n          ...,\n          [ 1.2980,  1.3154,  1.3154,  ...,  1.4200,  1.4374,  1.4548],\n          [ 1.3328,  1.3677,  1.3851,  ...,  1.4548,  1.4897,  1.5071],\n          [ 1.3502,  1.3851,  1.4200,  ...,  1.4722,  1.5245,  1.5594]]],\n\n\n        ...,\n\n\n        [[[-1.6384, -1.6042, -1.5870,  ..., -1.3815, -1.3815, -1.3987],\n          [-1.5357, -1.5014, -1.4843,  ..., -1.2617, -1.2617, -1.2788],\n          [-1.4500, -1.4329, -1.4329,  ..., -1.1589, -1.1760, -1.2103],\n          ...,\n          [-1.9124, -1.8953, -1.8782,  ..., -1.4500, -1.4672, -1.4843],\n          [-1.9124, -1.8953, -1.8953,  ..., -1.4672, -1.4843, -1.5014],\n          [-1.9124, -1.9124, -1.9124,  ..., -1.4843, -1.5014, -1.5014]],\n\n         [[-1.7381, -1.7381, -1.7381,  ..., -1.7906, -1.7906, -1.8256],\n          [-1.6856, -1.6681, -1.6681,  ..., -1.7206, -1.7206, -1.7381],\n          [-1.6331, -1.6331, -1.6506,  ..., -1.6856, -1.6856, -1.7031],\n          ...,\n          [-1.8256, -1.8081, -1.7906,  ..., -1.7556, -1.7731, -1.7731],\n          [-1.8256, -1.8081, -1.8081,  ..., -1.7731, -1.7731, -1.7731],\n          [-1.8256, -1.8256, -1.8256,  ..., -1.7731, -1.7731, -1.7731]],\n\n         [[-1.6476, -1.6302, -1.6127,  ..., -1.7870, -1.7870, -1.7870],\n          [-1.5779, -1.5604, -1.5604,  ..., -1.7522, -1.7522, -1.7522],\n          [-1.5430, -1.5430, -1.5604,  ..., -1.7522, -1.7522, -1.7696],\n          ...,\n          [-1.8044, -1.7870, -1.7696,  ..., -1.7696, -1.7870, -1.7870],\n          [-1.7870, -1.7696, -1.7870,  ..., -1.7870, -1.7870, -1.8044],\n          [-1.7696, -1.7696, -1.7696,  ..., -1.7870, -1.8044, -1.8044]]],\n\n\n        [[[-1.6384, -1.6042, -1.5870,  ..., -1.3815, -1.3815, -1.3987],\n          [-1.5357, -1.5014, -1.4843,  ..., -1.2617, -1.2617, -1.2788],\n          [-1.4500, -1.4329, -1.4329,  ..., -1.1589, -1.1760, -1.2103],\n          ...,\n          [-1.9124, -1.8953, -1.8782,  ..., -1.4500, -1.4672, -1.4843],\n          [-1.9124, -1.8953, -1.8953,  ..., -1.4672, -1.4843, -1.5014],\n          [-1.9124, -1.9124, -1.9124,  ..., -1.4843, -1.5014, -1.5014]],\n\n         [[-1.7381, -1.7381, -1.7381,  ..., -1.7906, -1.7906, -1.8256],\n          [-1.6856, -1.6681, -1.6681,  ..., -1.7206, -1.7206, -1.7381],\n          [-1.6331, -1.6331, -1.6506,  ..., -1.6856, -1.6856, -1.7031],\n          ...,\n          [-1.8256, -1.8081, -1.7906,  ..., -1.7556, -1.7731, -1.7731],\n          [-1.8256, -1.8081, -1.8081,  ..., -1.7731, -1.7731, -1.7731],\n          [-1.8256, -1.8256, -1.8256,  ..., -1.7731, -1.7731, -1.7731]],\n\n         [[-1.6476, -1.6302, -1.6127,  ..., -1.7870, -1.7870, -1.7870],\n          [-1.5779, -1.5604, -1.5604,  ..., -1.7522, -1.7522, -1.7522],\n          [-1.5430, -1.5430, -1.5604,  ..., -1.7522, -1.7522, -1.7696],\n          ...,\n          [-1.8044, -1.7870, -1.7696,  ..., -1.7696, -1.7870, -1.7870],\n          [-1.7870, -1.7696, -1.7870,  ..., -1.7870, -1.7870, -1.8044],\n          [-1.7696, -1.7696, -1.7696,  ..., -1.7870, -1.8044, -1.8044]]],\n\n\n        [[[-1.6384, -1.6042, -1.5870,  ..., -1.3815, -1.3815, -1.3987],\n          [-1.5357, -1.5014, -1.4843,  ..., -1.2617, -1.2617, -1.2788],\n          [-1.4500, -1.4329, -1.4329,  ..., -1.1589, -1.1760, -1.2103],\n          ...,\n          [-1.9124, -1.8953, -1.8782,  ..., -1.4500, -1.4672, -1.4843],\n          [-1.9124, -1.8953, -1.8953,  ..., -1.4672, -1.4843, -1.5014],\n          [-1.9124, -1.9124, -1.9124,  ..., -1.4843, -1.5014, -1.5014]],\n\n         [[-1.7381, -1.7381, -1.7381,  ..., -1.7906, -1.7906, -1.8256],\n          [-1.6856, -1.6681, -1.6681,  ..., -1.7206, -1.7206, -1.7381],\n          [-1.6331, -1.6331, -1.6506,  ..., -1.6856, -1.6856, -1.7031],\n          ...,\n          [-1.8256, -1.8081, -1.7906,  ..., -1.7556, -1.7731, -1.7731],\n          [-1.8256, -1.8081, -1.8081,  ..., -1.7731, -1.7731, -1.7731],\n          [-1.8256, -1.8256, -1.8256,  ..., -1.7731, -1.7731, -1.7731]],\n\n         [[-1.6476, -1.6302, -1.6127,  ..., -1.7870, -1.7870, -1.7870],\n          [-1.5779, -1.5604, -1.5604,  ..., -1.7522, -1.7522, -1.7522],\n          [-1.5430, -1.5430, -1.5604,  ..., -1.7522, -1.7522, -1.7696],\n          ...,\n          [-1.8044, -1.7870, -1.7696,  ..., -1.7696, -1.7870, -1.7870],\n          [-1.7870, -1.7696, -1.7870,  ..., -1.7870, -1.7870, -1.8044],\n          [-1.7696, -1.7696, -1.7696,  ..., -1.7870, -1.8044, -1.8044]]]]), type: <class 'torch.Tensor'>, valid types: <class 'bytes'>, <class 'bytearray'>, file-like object"
     ]
    }
   ],
   "source": [
    "test(test_loader, predictor, torch.nn.CrossEntropyLoss(), device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Delete endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'PyTorchModel' object has no attribute 'delete_endpoint'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-84-46ef5e5e8b7c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'PyTorchModel' object has no attribute 'delete_endpoint'"
     ]
    }
   ],
   "source": [
    "estimator.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
