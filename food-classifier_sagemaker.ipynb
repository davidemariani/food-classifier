{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the data\n",
    "\n",
    "The dataset is retrieved from the website http://data.vision.ee.ethz.ch/cvl/food-101.tar.gz using as a reference the following research paper\n",
    "\n",
    "\n",
    "> Lukas Bossard, Matthieu Guillaumin, Luc Van Gool - Food-101 – Mining Discriminative Components with Random Forests\n",
    "\n",
    "The Food-101 data set consists of images from Foodspotting [1]. Any use beyond\n",
    "   scientific fair use must be negociated with the respective picture owners\n",
    "   according to the Foodspotting terms of use [2].\n",
    "\n",
    "[1] http://www.foodspotting.com/\n",
    "[2] http://www.foodspotting.com/terms/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing essential modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from os import path\n",
    "import time\n",
    "from random import seed, choice\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deleting the data folder in case a cleaning is required\n",
    "#shutil.rmtree(\"../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the data will be downloaded and automatically extracted to the data folder ../data/food-101/images\n",
    "%mkdir ../data\n",
    "!wget -O ../data/food-101.tar.gz http://data.vision.ee.ethz.ch/cvl/food-101.tar.gz\n",
    "!tar -zxf ../data/food-101.tar.gz -C ../data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organise train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dividing into train and test set using the json metadata \n",
    "\n",
    "metafolder = \"../data/food-101/meta/\"\n",
    "train_meta = pd.read_json(path_or_buf = metafolder + \"train.json\")\n",
    "test_meta = pd.read_json(path_or_buf = metafolder + \"test.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Organising metdatada for training, testing and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected categories : ramen, carrot_cake, beef_carpaccio, strawberry_shortcake, escargots\n"
     ]
    }
   ],
   "source": [
    "#organising metadata for training, testing and validation\n",
    "validation_split = 0.2\n",
    "val_split_idx = int(np.floor(train_meta.shape[0]*validation_split))\n",
    "\n",
    "#folder with all the food images\n",
    "data_dir = \"../data/food-101/images/\"\n",
    "folders_sorted = sorted(os.listdir(data_dir))\n",
    "\n",
    "#number of categories to randomly select\n",
    "nc = 5\n",
    "\n",
    "#selecting a randomn subset of categories\n",
    "seed(42)\n",
    "\n",
    "selection = []\n",
    "while len(selection) < nc:\n",
    "    pick = choice(folders_sorted)\n",
    "    if pick not in set(selection):\n",
    "        selection.append(pick)\n",
    "        \n",
    "print(\"Selected categories : {}\".format(', '.join(map(str, selection))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘../data/s3_train_data’: File exists\n",
      "mkdir: cannot create directory ‘../data/s3_train_data/train_img’: File exists\n",
      "mkdir: cannot create directory ‘../data/s3_train_data/valid_img’: File exists\n",
      "mkdir: cannot create directory ‘../data/test_img’: File exists\n",
      "3000 images used for training\n",
      "750 images used for validation\n",
      "1250 images used for testing\n"
     ]
    }
   ],
   "source": [
    "#create folder with data to upload to s3\n",
    "%mkdir ../data/s3_train_data\n",
    "%mkdir ../data/s3_train_data/train_img \n",
    "%mkdir ../data/s3_train_data/valid_img\n",
    "%mkdir ../data/test_img\n",
    "\n",
    "train_meta = train_meta[selection].iloc[:train_meta.shape[0] - val_split_idx]\n",
    "\n",
    "valid_meta = train_meta[selection].iloc[train_meta.shape[0] - val_split_idx:]\n",
    "\n",
    "test_meta = test_meta[selection]\n",
    "\n",
    "#Setting train, validation and test set target folder\n",
    "#target folder - train\n",
    "trainfolder = \"../data/s3_train_data/train_img/\"\n",
    "\n",
    "#target folder - validation\n",
    "validfolder = \"../data/s3_train_data/valid_img/\"\n",
    "\n",
    "#target folder -test\n",
    "testfolder = \"../data/test_img/\"\n",
    "\n",
    "print(\"{} images used for training\".format(train_meta.shape[0]*train_meta.shape[1]))\n",
    "print(\"{} images used for validation\".format(valid_meta.shape[0]*valid_meta.shape[1]))\n",
    "print(\"{} images used for testing\".format(test_meta.shape[0]*test_meta.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dividing into train and test set using the json metadata \n",
    "\n",
    "def organise_files_from_df(df, datafolder, datatarget):\n",
    "    \"\"\"\n",
    "    This function moves files contained in a folder (datafolder) to a target path (datatarget),\n",
    "    based on the information contained on a dataframe (df) where each column corresponds to a \n",
    "    class name (sub-folder). Every column of the dataset contains a list of filenames to be moved.\n",
    "    \"\"\"\n",
    "    \n",
    "    #creating target folder\n",
    "    if not path.exists(datatarget):\n",
    "        os.mkdir(datatarget)\n",
    "    \n",
    "    #iterating through dataframe columns ( =  labels)\n",
    "    for label in list(df.columns):\n",
    "        \n",
    "        #create folder\n",
    "        foldername = datatarget + str(label)\n",
    "        \n",
    "        if not path.exists(foldername):\n",
    "            os.mkdir(foldername)\n",
    "        \n",
    "        #move each file\n",
    "        for file in list(df[label]):\n",
    "            \n",
    "            fileoriginal =  datafolder + file + \".jpg\"\n",
    "            filetarget = datatarget +\"/\" + file + \".jpg\"\n",
    "            \n",
    "            try:\n",
    "                if not path.exists(filetarget):\n",
    "                    shutil.copyfile(fileoriginal, filetarget)\n",
    "\n",
    "            except FileNotFoundError:\n",
    "                print(\"File {} not found!\".format(file))\n",
    "                pass\n",
    "\n",
    "#origin folder\n",
    "imagefolder = \"../data/food-101/images/\"\n",
    "\n",
    "\n",
    "organise_files_from_df(train_meta, imagefolder, trainfolder)\n",
    "\n",
    "organise_files_from_df(valid_meta, imagefolder, validfolder)\n",
    "\n",
    "organise_files_from_df(test_meta, imagefolder, testfolder)\n",
    "\n",
    "#to delete the origin folder, uncomment the line below\n",
    "#shutil.rmtree(imagefolder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data to S3\n",
    "\n",
    ">The below cells load in some AWS SageMaker libraries, starts a SageMaker session and creates a default bucket. After creating this bucket, it upload the locally stored data to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "# session and role\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# create an S3 bucket\n",
    "bucket = sagemaker_session.default_bucket()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Upload training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data uploaded to s3 after 296.4869067668915 seconds\n"
     ]
    }
   ],
   "source": [
    "prefix = \"food-classifier\"\n",
    "datafolder = \"../data/s3_train_data\"\n",
    "# upload all data to S3\n",
    "\n",
    "#this is slow!\n",
    "start = time.time()\n",
    "input_data = sagemaker_session.upload_data(path=datafolder, bucket=bucket, key_prefix=prefix)\n",
    "end = time.time()\n",
    "\n",
    "print(\"Data uploaded to s3 after {} seconds\".format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All good!\n"
     ]
    }
   ],
   "source": [
    "# check that data is in S3 bucket\n",
    "empty_check = []\n",
    "for obj in boto3.resource('s3').Bucket(bucket).objects.all():\n",
    "    empty_check.append(obj.key)\n",
    "    #print(obj.key)\n",
    "\n",
    "assert len(empty_check) !=0, 'S3 bucket is empty.'\n",
    "print('All good!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Checking model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorchvision.models\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mmodels\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m#importing pretrained ResNet for transfer learning\u001b[39;49;00m\n",
      "ResNetTransfer = models.resnet50(pretrained=\u001b[36mTrue\u001b[39;49;00m) \u001b[37m#.load_state_dict(torch.load(\"resnet50_base.pt\"))\u001b[39;49;00m\n"
     ]
    }
   ],
   "source": [
    "!pygmentize pytorch_source/model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Checking train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch.optim\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36moptim\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch.nn\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnn\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch.utils.data\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtorchvision\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m datasets\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorchvision.transforms\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mtransforms\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch.optim\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36moptim\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtorch.optim.lr_scheduler\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m ReduceLROnPlateau\n",
      "\n",
      "\u001b[37m# imports the model in model.py by name\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mmodel\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m ResNetTransfer\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmodel_fn\u001b[39;49;00m(model_dir):\n",
      "    \u001b[33m\"\"\"Load the PyTorch model from the `model_dir` directory.\"\"\"\u001b[39;49;00m\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mLoading model.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[37m# First, load the parameters used to create the model.\u001b[39;49;00m\n",
      "    model_info = {}\n",
      "    model_info_path = os.path.join(model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel_info.pth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(model_info_path, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\n",
      "        model_info = torch.load(f)\n",
      "\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mmodel_info: {}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(model_info))\n",
      "\n",
      "    \u001b[37m# Determine the device and construct the model.\u001b[39;49;00m\n",
      "    device = torch.device(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m torch.cuda.is_available() \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mUsing device {}.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(device))\n",
      "\n",
      "    model = ResNetTransfer\n",
      "\n",
      "    \u001b[37m#freezing the parameters\u001b[39;49;00m\n",
      "    \u001b[34mfor\u001b[39;49;00m param \u001b[35min\u001b[39;49;00m model.parameters():\n",
      "        param.requires_grad = \u001b[36mFalse\u001b[39;49;00m\n",
      "    model.fc = nn.Linear(model.fc.in_features, model_info[\u001b[33m\"\u001b[39;49;00m\u001b[33mn_classes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "\n",
      "    \u001b[37m# Load the stored model parameters.\u001b[39;49;00m\n",
      "    model_path = os.path.join(model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel.pth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(model_path, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\n",
      "        model.load_state_dict(torch.load(f))\n",
      "\n",
      "    \u001b[37m# set to eval mode, could use no_grad\u001b[39;49;00m\n",
      "    model.to(device).eval()\n",
      "\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mDone loading model.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \u001b[34mreturn\u001b[39;49;00m model\n",
      "\n",
      "\n",
      "\u001b[37m# Gets prepared training data for the Dataloaders\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_get_train_data_loader\u001b[39;49;00m(img_short_side_resize, img_input_size, norm_mean, norm_std,\n",
      "                           shuffle, num_workers, batch_size, datadir, trainfolder, validfolder):\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mGet train data loader.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "    transform_train = transforms.Compose([\n",
      "                    transforms.Resize(img_short_side_resize),\n",
      "                    transforms.ColorJitter(brightness=\u001b[34m0.2\u001b[39;49;00m, contrast=\u001b[34m0.2\u001b[39;49;00m, saturation=\u001b[34m0.2\u001b[39;49;00m, hue=\u001b[34m0.1\u001b[39;49;00m),\n",
      "                    transforms.RandomHorizontalFlip(),\n",
      "                    transforms.RandomResizedCrop(img_input_size, scale=(\u001b[34m0.08\u001b[39;49;00m,\u001b[34m1\u001b[39;49;00m), ratio=(\u001b[34m1\u001b[39;49;00m,\u001b[34m1\u001b[39;49;00m)), \n",
      "                    transforms.ToTensor(),\n",
      "                    transforms.Normalize(mean = norm_mean, std = norm_std)])\n",
      "    transform_test = transforms.Compose([\n",
      "                        transforms.Resize(img_input_size),  \n",
      "                        transforms.FiveCrop(img_input_size),\n",
      "                        transforms.Lambda(\u001b[34mlambda\u001b[39;49;00m crops: torch.stack([transforms.Compose([\n",
      "                        transforms.ToTensor(),\n",
      "                        transforms.Normalize(mean = norm_mean, std = norm_std)])(crop) \u001b[34mfor\u001b[39;49;00m crop \u001b[35min\u001b[39;49;00m crops]))])\n",
      "\n",
      "    train_data = datasets.ImageFolder(datadir + trainfolder, transform_train)\n",
      "    valid_data = datasets.ImageFolder(datadir + validfolder, transform_test)\n",
      "\n",
      "    \u001b[37m# Create the data loaders\u001b[39;49;00m\n",
      "    data = {\u001b[33m\"\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m : train_data, \u001b[33m\"\u001b[39;49;00m\u001b[33mval\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:valid_data}\n",
      "\n",
      "    train_loader = torch.utils.data.DataLoader(data[\u001b[33m\"\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], batch_size=batch_size, num_workers=num_workers, shuffle=shuffle, pin_memory=\u001b[36mTrue\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[37m#### --- NOTE on num_workers if using 5crop and batch_size for testing --- ###\u001b[39;49;00m\n",
      "    \u001b[37m# If using the 5crop test time augmentation, num_workers = 0 (an error is raised otherwise) \u001b[39;49;00m\n",
      "    \u001b[37m# batch_size needs to be reduced during testing due to memory requirements\u001b[39;49;00m\n",
      "    valid_loader = torch.utils.data.DataLoader(data[\u001b[33m\"\u001b[39;49;00m\u001b[33mval\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], batch_size=\u001b[36mint\u001b[39;49;00m(np.floor(batch_size/\u001b[34m5\u001b[39;49;00m)), num_workers=\u001b[34m0\u001b[39;49;00m, shuffle=shuffle, pin_memory=\u001b[36mTrue\u001b[39;49;00m)\n",
      "\n",
      "    loaders_transfer = {\u001b[33m\"\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m : train_loader, \u001b[33m\"\u001b[39;49;00m\u001b[33mval\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:valid_loader}\n",
      "\n",
      "    \u001b[34mreturn\u001b[39;49;00m loaders_transfer\n",
      "\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtime\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mdatetime\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtrain_epoch\u001b[39;49;00m(model,train_loader,optimizer,criterion,device):\n",
      "    \u001b[33m\"\"\"\u001b[39;49;00m\n",
      "\u001b[33m    train steps at each epoch\u001b[39;49;00m\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\n",
      "    train_loss = \u001b[34m0.0\u001b[39;49;00m\n",
      "    \n",
      "    model.train()\n",
      "    \n",
      "    \u001b[34mfor\u001b[39;49;00m batch_idx, (data, target) \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(train_loader):\n",
      "        \n",
      "        data, target = data.to(device), target.to(device) \u001b[37m# move to GPU\u001b[39;49;00m\n",
      "        \n",
      "        optimizer.zero_grad() \u001b[37m# set gradients to 0\u001b[39;49;00m\n",
      "        \n",
      "        output = model(data) \u001b[37m# get output\u001b[39;49;00m\n",
      "        \n",
      "        loss = criterion(output, target) \u001b[37m# calculate loss\u001b[39;49;00m\n",
      "        train_loss += loss.item() * data.size(\u001b[34m0\u001b[39;49;00m)\n",
      "        \n",
      "        loss.backward() \u001b[37m# calculate gradients\u001b[39;49;00m\n",
      "        \n",
      "        optimizer.step() \u001b[37m# take step\u001b[39;49;00m\n",
      "        \n",
      "    train_loss = train_loss / \u001b[36mlen\u001b[39;49;00m(train_loader.dataset)\n",
      "    \u001b[34mreturn\u001b[39;49;00m model, train_loss\n",
      "        \n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mvalid_epoch\u001b[39;49;00m(model, valid_loader, criterion, device, fivecrop):\n",
      "    \u001b[33m\"\"\"\u001b[39;49;00m\n",
      "\u001b[33m    validation prediction steps at each epoch\u001b[39;49;00m\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\n",
      "    valid_loss = \u001b[34m0.0\u001b[39;49;00m\n",
      "    \n",
      "    model.eval()\n",
      "    \n",
      "    \u001b[34mwith\u001b[39;49;00m torch.no_grad():\n",
      "        \u001b[34mfor\u001b[39;49;00m data, target \u001b[35min\u001b[39;49;00m valid_loader:\n",
      "            \n",
      "            data, target = data.to(device), target.to(device) \u001b[37m# move to GPU\u001b[39;49;00m\n",
      "            \n",
      "            \u001b[37m# if we do test time augmentation with 5crop we'll have an extra dimension in our tensor\u001b[39;49;00m\n",
      "            \u001b[34mif\u001b[39;49;00m fivecrop == \u001b[33m\"\u001b[39;49;00m\u001b[33mmean\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\n",
      "                bs, ncrops, c, h, w = data.size()\n",
      "                output = model(data.view(-\u001b[34m1\u001b[39;49;00m, c, h, w)) \u001b[37m# fuse batch size and ncrops\u001b[39;49;00m\n",
      "                output = output.view(bs, ncrops, -\u001b[34m1\u001b[39;49;00m).mean(\u001b[34m1\u001b[39;49;00m)\n",
      "            \u001b[34melif\u001b[39;49;00m fivecrop == \u001b[33m\"\u001b[39;49;00m\u001b[33mmax\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\n",
      "                bs, ncrops, c, h, w = data.size()\n",
      "                output = model(data.view(-\u001b[34m1\u001b[39;49;00m, c, h, w)) \u001b[37m# fuse batch size and ncrops\u001b[39;49;00m\n",
      "                output = output.view(bs, ncrops, -\u001b[34m1\u001b[39;49;00m).max(\u001b[34m1\u001b[39;49;00m)[\u001b[34m0\u001b[39;49;00m]\n",
      "            \u001b[34melse\u001b[39;49;00m:\n",
      "                output = model(data)\n",
      "                \n",
      "            \u001b[37m## update the average validation loss\u001b[39;49;00m\n",
      "            loss = criterion(output, target)\n",
      "            valid_loss += loss.item() * data.size(\u001b[34m0\u001b[39;49;00m)\n",
      "            \n",
      "    valid_loss = valid_loss / \u001b[36mlen\u001b[39;49;00m(valid_loader.dataset) \n",
      "    \u001b[34mreturn\u001b[39;49;00m valid_loss\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtrain\u001b[39;49;00m(n_epochs, loaders, model, optimizer, criterion, device, path_model, fivecrop = \u001b[36mNone\u001b[39;49;00m, lr_scheduler = \u001b[36mNone\u001b[39;49;00m):\n",
      "    \u001b[33m\"\"\"\u001b[39;49;00m\n",
      "\u001b[33m    model training\u001b[39;49;00m\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\n",
      "    \n",
      "    \u001b[37m# initialize tracker for minimum validation loss\u001b[39;49;00m\n",
      "    valid_loss_min = np.Inf \n",
      "    train_loss = []\n",
      "    valid_loss = []\n",
      "    \n",
      "    time_start = time.time()\n",
      "    best_epoch = \u001b[34m0\u001b[39;49;00m\n",
      "    \n",
      "    \u001b[34mfor\u001b[39;49;00m epoch \u001b[35min\u001b[39;49;00m \u001b[36mrange\u001b[39;49;00m(\u001b[34m1\u001b[39;49;00m, n_epochs+\u001b[34m1\u001b[39;49;00m):\n",
      "        \n",
      "        time_start_epoch = time.time()  \n",
      "        \n",
      "        \u001b[37m# train current epoch\u001b[39;49;00m\n",
      "        model, train_loss_epoch = train_epoch(model, loaders[\u001b[33m\"\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], optimizer, criterion, device) \n",
      "        train_loss.append(train_loss_epoch)   \n",
      "        \n",
      "        \u001b[37m# validate current epoch\u001b[39;49;00m\n",
      "        valid_loss_epoch = valid_epoch(model,loaders[\u001b[33m\"\u001b[39;49;00m\u001b[33mval\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m],criterion,device,fivecrop)\n",
      "        \n",
      "        \u001b[37m# learning rate scheduler\u001b[39;49;00m\n",
      "        \u001b[34mif\u001b[39;49;00m lr_scheduler \u001b[35mis\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m \u001b[36mNone\u001b[39;49;00m:\n",
      "            lr_scheduler.step(valid_loss_epoch)\n",
      "        valid_loss.append(valid_loss_epoch)  \n",
      "        \n",
      "        \u001b[34mif\u001b[39;49;00m valid_loss_epoch <= valid_loss_min: \u001b[37m# save if validation loss is the lowest so far\u001b[39;49;00m\n",
      "            torch.save(model.state_dict(), path_model)\n",
      "            valid_loss_min = valid_loss_epoch \n",
      "            best_epoch = epoch\n",
      "            \n",
      "        \u001b[37m# print epoch stats\u001b[39;49;00m\n",
      "        currentDT = datetime.datetime.now()\n",
      "        exact_time =  \u001b[36mstr\u001b[39;49;00m(currentDT.hour) + \u001b[33m\"\u001b[39;49;00m\u001b[33m:\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m + \u001b[36mstr\u001b[39;49;00m(currentDT.minute) + \u001b[33m\"\u001b[39;49;00m\u001b[33m:\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m + \u001b[36mstr\u001b[39;49;00m(currentDT.second)\n",
      "        \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mEpoch {} done in {:.2f} seconds at {}. \u001b[39;49;00m\u001b[33m\\t\u001b[39;49;00m\u001b[33mTraining Loss: {:.3f} \u001b[39;49;00m\u001b[33m\\t\u001b[39;49;00m\u001b[33mValidation Loss: {:.3f}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format( \n",
      "            epoch,             \n",
      "            time.time() - time_start_epoch,\n",
      "            exact_time,\n",
      "            train_loss_epoch,\n",
      "            valid_loss_epoch\n",
      "            ))   \n",
      "        \n",
      "    \u001b[37m# print final statistics    \u001b[39;49;00m\n",
      "    \u001b[34mprint\u001b[39;49;00m(f\u001b[33m\"\u001b[39;49;00m\u001b[33m{n_epochs} epochs trained in {(time.time() - time_start):.3f} seconds. \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \n",
      "    \n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mBest model obtained at epoch {} with minimum validation loss : {:.3f}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(best_epoch, valid_loss_min))\n",
      "    \n",
      "    \u001b[37m# Load best config\u001b[39;49;00m\n",
      "    model.load_state_dict(torch.load(path_model))\n",
      "    \n",
      "    \u001b[34mreturn\u001b[39;49;00m model\n",
      "\n",
      "\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m'\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\n",
      "    \n",
      "    \u001b[37m# Parameters settings\u001b[39;49;00m\n",
      "    parser = argparse.ArgumentParser()\n",
      "\n",
      "    \u001b[37m# SageMaker parameters\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--output-data-dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_OUTPUT_DATA_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--model-dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--data-dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CHANNEL_TRAINING\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    \n",
      "    \u001b[37m#Data preparation parameters\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--img_short_side_resize\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m256\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mI\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mResize to (default: 256)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--img_input_size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m224\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mI\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mImage input to ResNet (default: 224)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--shuffle\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mbool\u001b[39;49;00m, default=\u001b[36mTrue\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mI\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mDataloader shuffle (default: True)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--num_workers\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m16\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mI\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mnumber of workers in data preparation (default: 16)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--trainfolder\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m'\u001b[39;49;00m\u001b[33m/train_img\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mI\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mName of the folder containing training data (default: train_img)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--validfolder\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m'\u001b[39;49;00m\u001b[33m/valid_img\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mI\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mName of the folder containing validation data (default: valid_img)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[37m# Training Parameters\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--batch_size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m64\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33minput batch size for training (default: 64)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--n_epochs\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m10\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mnumber of epochs to train (default: 10)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--seed\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m42\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mS\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mrandom seed (default: 42)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--lr\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m3e-4\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mLR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mlearning rate (default: 3e-4)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \n",
      "    \u001b[37m# Model Parameters\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--n_classes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m2\u001b[39;49;00m, metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mM\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33moutput dimension of the model (int - default: 2)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \n",
      "    \n",
      "    \u001b[37m# args holds all passed-in arguments\u001b[39;49;00m\n",
      "    args = parser.parse_args()\n",
      "\n",
      "    device = torch.device(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m torch.cuda.is_available() \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mUsing device {}.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(device))\n",
      "    \u001b[34mif\u001b[39;49;00m torch.cuda.is_available():\n",
      "        \u001b[34mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mUsing\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,torch.cuda.get_device_name(device))\n",
      "\n",
      "    torch.manual_seed(args.seed)\n",
      "\n",
      "    \u001b[37m# Load the training data.\u001b[39;49;00m\n",
      "    \u001b[37m#setting norm_mean and norm_std\u001b[39;49;00m\n",
      "    norm_mean = [\u001b[34m0.485\u001b[39;49;00m, \u001b[34m0.456\u001b[39;49;00m, \u001b[34m0.406\u001b[39;49;00m]\n",
      "    norm_std = [\u001b[34m0.229\u001b[39;49;00m, \u001b[34m0.224\u001b[39;49;00m, \u001b[34m0.225\u001b[39;49;00m]\n",
      "\n",
      "    train_loader = _get_train_data_loader(args.img_short_side_resize, \n",
      "                                          args.img_input_size,\n",
      "                                          norm_mean,\n",
      "                                          norm_std,\n",
      "                                          args.shuffle,\n",
      "                                          args.num_workers,\n",
      "                                          args.batch_size, \n",
      "                                          args.data_dir,\n",
      "                                          args.trainfolder,\n",
      "                                          args.validfolder)\n",
      "\n",
      "    \u001b[37m# Load the ResNet model\u001b[39;49;00m\n",
      "    model = ResNetTransfer\n",
      "\n",
      "    \u001b[37m#freezing the parameters\u001b[39;49;00m\n",
      "    \u001b[34mfor\u001b[39;49;00m param \u001b[35min\u001b[39;49;00m model.parameters():\n",
      "        param.requires_grad = \u001b[36mFalse\u001b[39;49;00m\n",
      "        \n",
      "    \u001b[37m# Replacing the last layer with a fully connected layer to retrain\u001b[39;49;00m\n",
      "    model.fc = nn.Linear(model.fc.in_features, args.n_classes) \n",
      "\n",
      "    \u001b[37m# Initialize the weights of the new layer\u001b[39;49;00m\n",
      "    nn.init.kaiming_normal_(model.fc.weight, nonlinearity=\u001b[33m'\u001b[39;49;00m\u001b[33mrelu\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[37m# Transfer to GPU \u001b[39;49;00m\n",
      "    model = model.to(device)\n",
      "\n",
      "    \u001b[37m## Optimizer and loss function for training\u001b[39;49;00m\n",
      "    criterion_transfer = nn.CrossEntropyLoss()\n",
      "    optimizer_transfer = optim.Adam(model.parameters(),args.lr) \n",
      "    scheduler_transfer = ReduceLROnPlateau(optimizer_transfer, \u001b[33m'\u001b[39;49;00m\u001b[33mmin\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, verbose = \u001b[36mTrue\u001b[39;49;00m, factor = \u001b[34m0.5\u001b[39;49;00m, patience = \u001b[34m7\u001b[39;49;00m)\n",
      "    \n",
      "\n",
      "    \u001b[37m# Trains the model \u001b[39;49;00m\n",
      "    train(args.n_epochs, \n",
      "          train_loader, \n",
      "          model, \n",
      "          optimizer_transfer, \n",
      "          criterion_transfer, \n",
      "          device, \n",
      "          os.path.join(args.model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel.pth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m), \n",
      "          fivecrop = \u001b[33m\"\u001b[39;49;00m\u001b[33mmean\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \n",
      "          lr_scheduler = scheduler_transfer)\n",
      "\n",
      "    \u001b[37m# Keep the keys of this dictionary as they are \u001b[39;49;00m\n",
      "    model_info_path = os.path.join(args.model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel_info.pth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(model_info_path, \u001b[33m'\u001b[39;49;00m\u001b[33mwb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\n",
      "        model_info = {\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33mn_classes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: args.n_classes,\n",
      "        }\n",
      "        torch.save(model_info, f)\n",
      "        \n",
      "    \u001b[37m# Save the model parameters\u001b[39;49;00m\n",
      "    model_path = os.path.join(args.model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel.pth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(model_path, \u001b[33m'\u001b[39;49;00m\u001b[33mwb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\n",
      "        torch.save(model.state_dict(), f)\n"
     ]
    }
   ],
   "source": [
    "!pygmentize pytorch_source/train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Create pytorch estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import a PyTorch wrapper\n",
    "from sagemaker.pytorch import PyTorch, PyTorchModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify an output path\n",
    "# prefix is specified above\n",
    "output_path = 's3://{}/{}'.format(bucket, prefix)\n",
    "\n",
    "# instantiate a pytorch estimator\n",
    "estimator = PyTorch(entry_point='train.py',\n",
    "                    source_dir='pytorch_source', \n",
    "                    role=role,\n",
    "                    framework_version= '1.1.0', #'1.3.1',\n",
    "                    train_instance_count=1,\n",
    "                    train_instance_type='ml.p2.xlarge',\n",
    "                    output_path=output_path,\n",
    "                    sagemaker_session=sagemaker_session,\n",
    "                    hyperparameters={\n",
    "                        'n_classes': nc + 1,  # num of classes for the fully connected layer at the end of the network (defined on the first cells)\n",
    "                        'n_epochs': 3,\n",
    "                        'img_short_side_resize':256,\n",
    "                        'img_input_size':224,\n",
    "                        'num_workers':16,\n",
    "                        'batch_size':64\n",
    "                    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.fit({'training': input_data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Deploy the model for testing\n",
    "\n",
    "The model will be tested by first deploying it and then sending the testing data to the deployed endpoint\n",
    "\n",
    "The function that loads the saved model is called `model_fn()` and takes as its only parameter a path to the directory where the model artifacts are stored. This function must also be present in the python file specified as the entry point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import RealTimePredictor\n",
    "\n",
    "class ImgPredictor(RealTimePredictor):\n",
    "    def __init__(self, endpoint_name, sagemaker_session):\n",
    "        super(ImgPredictor, self).__init__(endpoint_name, sagemaker_session, content_type='application/x-image') #, accept='application/json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in case we want to retrieve the model from s3\n",
    "estimator = PyTorchModel(model_data='s3://sagemaker-eu-central-1-515611759963/food-classifier/sagemaker-pytorch-2020-03-03-15-53-46-306/output/model.tar.gz', \n",
    "                             role=role,\n",
    "                             source_dir='pytorch_source',\n",
    "                             entry_point='deploy.py',\n",
    "                            predictor_cls = ImgPredictor,\n",
    "                           framework_version = '1.1.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------!"
     ]
    }
   ],
   "source": [
    "predictor = estimator.deploy(instance_type='ml.p2.xlarge', initial_instance_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictor = sagemaker.predictor.RealTimePredictor(\n",
    "#    endpoint='sagemaker-pytorch-2020-03-05-13-01-45-046',\n",
    "#    sagemaker_session = sagemaker_session,\n",
    "#    content_type='application/x-npy',\n",
    "#    accept='application/x-npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.ImgPredictor at 0x7f547994b208>"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Transformers set-up for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "#Norm values\n",
    "norm_mean = [0.485, 0.456, 0.406]\n",
    "norm_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "#Img size parameters\n",
    "img_short_side_resize = 256\n",
    "img_input_size = 224\n",
    "\n",
    "import os\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "                    transforms.Resize(img_input_size),  \n",
    "                    transforms.FiveCrop(img_input_size),\n",
    "                    transforms.Lambda(lambda crops: torch.stack([transforms.Compose([\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean = norm_mean, std = norm_std)])(crop) for crop in crops]))])\n",
    "\n",
    "test_data = datasets.ImageFolder(testfolder, transform_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardTransform\n",
       "Transform: Compose(\n",
       "               Resize(size=224, interpolation=PIL.Image.BILINEAR)\n",
       "               FiveCrop(size=(224, 224))\n",
       "               Lambda()\n",
       "           )"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "image = Image.open(test_data.imgs[0][0]).convert('RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255\n"
     ]
    }
   ],
   "source": [
    "with open(test_data.imgs[0][0], \"rb\") as image:\n",
    "    f = image.read()\n",
    "    b = bytearray(f)\n",
    "    print(b[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = predictor.predict(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bytes"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "shuffle = True\n",
    "num_workers = 16\n",
    "batch_size = 64\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=int(np.floor(batch_size/5)), num_workers=0, shuffle=shuffle, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Testing function implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO BE IMPLEMENTED PROPERLY\n",
    "\n",
    "def test(loader, model, criterion, device):\n",
    "    \"\"\"\n",
    "    test function\n",
    "    \"\"\"\n",
    "    \n",
    "    test_loss = 0.\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "    \n",
    "    #model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(loader):\n",
    "            \n",
    "            data, target = data.to(device), target.to(device) # move to GPU\n",
    "            \n",
    "            bs, ncrops, c, h, w = data.size()\n",
    "            \n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model.predict(data.view(-1, c, h, w)) # fuse batch size and ncrops\n",
    "            output = output.view(bs, ncrops, -1).mean(1)    \n",
    "            \n",
    "            loss = criterion(output, target) # calculate the loss\n",
    "            \n",
    "            test_loss = test_loss + ((1 / (batch_idx + 1)) * (loss.data - test_loss)) # update average test loss \n",
    "            \n",
    "            pred = output.data.max(1, keepdim=True)[1] # convert output probabilities to predicted class\n",
    "            \n",
    "            # compare predictions to true label\n",
    "            correct += np.sum(np.squeeze(pred.eq(target.data.view_as(pred))).cpu().numpy()) \n",
    "            total += data.size(0)            \n",
    "            \n",
    "    print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "    print('\\nTest Accuracy: %2d%% (%2d/%2d)' % (\n",
    "        100. * correct / total, correct, total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cpu.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "print(\"Using device {}.\".format(device))\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Using\",torch.cuda.get_device_name(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictor.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParamValidationError",
     "evalue": "Parameter validation failed:\nInvalid type for parameter Body, value: tensor([[[[-1.9124, -1.8953, -1.8439,  ..., -2.0152, -2.0494, -2.0665],\n          [-1.8953, -1.8610, -1.8610,  ..., -2.0323, -2.0665, -2.0494],\n          [-1.8268, -1.8439, -1.8610,  ..., -2.0494, -2.0837, -2.0665],\n          ...,\n          [ 0.9988,  0.9988,  1.0159,  ...,  1.1358,  1.1529,  1.1700],\n          [ 0.9988,  1.0331,  1.0331,  ...,  1.1700,  1.2043,  1.2214],\n          [ 0.9817,  0.9988,  1.0502,  ...,  1.1872,  1.2385,  1.2728]],\n\n         [[-1.8431, -1.8256, -1.7731,  ..., -1.9307, -1.9657, -1.9832],\n          [-1.7906, -1.7731, -1.7556,  ..., -1.9482, -1.9832, -1.9657],\n          [-1.6856, -1.7031, -1.7206,  ..., -1.9657, -2.0007, -1.9832],\n          ...,\n          [ 1.0805,  1.0980,  1.0980,  ...,  1.2206,  1.2381,  1.2556],\n          [ 1.0630,  1.0980,  1.0980,  ...,  1.2556,  1.2906,  1.3081],\n          [ 1.0455,  1.0630,  1.1155,  ...,  1.2731,  1.3256,  1.3606]],\n\n         [[-1.6999, -1.6650, -1.6127,  ..., -1.6650, -1.6999, -1.7173],\n          [-1.6824, -1.6476, -1.6302,  ..., -1.6824, -1.7173, -1.6999],\n          [-1.6127, -1.6302, -1.6476,  ..., -1.6999, -1.7347, -1.7173],\n          ...,\n          [ 1.2980,  1.3154,  1.3154,  ...,  1.4200,  1.4374,  1.4548],\n          [ 1.3328,  1.3677,  1.3851,  ...,  1.4548,  1.4897,  1.5071],\n          [ 1.3502,  1.3851,  1.4200,  ...,  1.4722,  1.5245,  1.5594]]],\n\n\n        [[[-1.9124, -1.8953, -1.8439,  ..., -2.0152, -2.0494, -2.0665],\n          [-1.8953, -1.8610, -1.8610,  ..., -2.0323, -2.0665, -2.0494],\n          [-1.8268, -1.8439, -1.8610,  ..., -2.0494, -2.0837, -2.0665],\n          ...,\n          [ 0.9988,  0.9988,  1.0159,  ...,  1.1358,  1.1529,  1.1700],\n          [ 0.9988,  1.0331,  1.0331,  ...,  1.1700,  1.2043,  1.2214],\n          [ 0.9817,  0.9988,  1.0502,  ...,  1.1872,  1.2385,  1.2728]],\n\n         [[-1.8431, -1.8256, -1.7731,  ..., -1.9307, -1.9657, -1.9832],\n          [-1.7906, -1.7731, -1.7556,  ..., -1.9482, -1.9832, -1.9657],\n          [-1.6856, -1.7031, -1.7206,  ..., -1.9657, -2.0007, -1.9832],\n          ...,\n          [ 1.0805,  1.0980,  1.0980,  ...,  1.2206,  1.2381,  1.2556],\n          [ 1.0630,  1.0980,  1.0980,  ...,  1.2556,  1.2906,  1.3081],\n          [ 1.0455,  1.0630,  1.1155,  ...,  1.2731,  1.3256,  1.3606]],\n\n         [[-1.6999, -1.6650, -1.6127,  ..., -1.6650, -1.6999, -1.7173],\n          [-1.6824, -1.6476, -1.6302,  ..., -1.6824, -1.7173, -1.6999],\n          [-1.6127, -1.6302, -1.6476,  ..., -1.6999, -1.7347, -1.7173],\n          ...,\n          [ 1.2980,  1.3154,  1.3154,  ...,  1.4200,  1.4374,  1.4548],\n          [ 1.3328,  1.3677,  1.3851,  ...,  1.4548,  1.4897,  1.5071],\n          [ 1.3502,  1.3851,  1.4200,  ...,  1.4722,  1.5245,  1.5594]]],\n\n\n        [[[-1.9124, -1.8953, -1.8439,  ..., -2.0152, -2.0494, -2.0665],\n          [-1.8953, -1.8610, -1.8610,  ..., -2.0323, -2.0665, -2.0494],\n          [-1.8268, -1.8439, -1.8610,  ..., -2.0494, -2.0837, -2.0665],\n          ...,\n          [ 0.9988,  0.9988,  1.0159,  ...,  1.1358,  1.1529,  1.1700],\n          [ 0.9988,  1.0331,  1.0331,  ...,  1.1700,  1.2043,  1.2214],\n          [ 0.9817,  0.9988,  1.0502,  ...,  1.1872,  1.2385,  1.2728]],\n\n         [[-1.8431, -1.8256, -1.7731,  ..., -1.9307, -1.9657, -1.9832],\n          [-1.7906, -1.7731, -1.7556,  ..., -1.9482, -1.9832, -1.9657],\n          [-1.6856, -1.7031, -1.7206,  ..., -1.9657, -2.0007, -1.9832],\n          ...,\n          [ 1.0805,  1.0980,  1.0980,  ...,  1.2206,  1.2381,  1.2556],\n          [ 1.0630,  1.0980,  1.0980,  ...,  1.2556,  1.2906,  1.3081],\n          [ 1.0455,  1.0630,  1.1155,  ...,  1.2731,  1.3256,  1.3606]],\n\n         [[-1.6999, -1.6650, -1.6127,  ..., -1.6650, -1.6999, -1.7173],\n          [-1.6824, -1.6476, -1.6302,  ..., -1.6824, -1.7173, -1.6999],\n          [-1.6127, -1.6302, -1.6476,  ..., -1.6999, -1.7347, -1.7173],\n          ...,\n          [ 1.2980,  1.3154,  1.3154,  ...,  1.4200,  1.4374,  1.4548],\n          [ 1.3328,  1.3677,  1.3851,  ...,  1.4548,  1.4897,  1.5071],\n          [ 1.3502,  1.3851,  1.4200,  ...,  1.4722,  1.5245,  1.5594]]],\n\n\n        ...,\n\n\n        [[[-1.6384, -1.6042, -1.5870,  ..., -1.3815, -1.3815, -1.3987],\n          [-1.5357, -1.5014, -1.4843,  ..., -1.2617, -1.2617, -1.2788],\n          [-1.4500, -1.4329, -1.4329,  ..., -1.1589, -1.1760, -1.2103],\n          ...,\n          [-1.9124, -1.8953, -1.8782,  ..., -1.4500, -1.4672, -1.4843],\n          [-1.9124, -1.8953, -1.8953,  ..., -1.4672, -1.4843, -1.5014],\n          [-1.9124, -1.9124, -1.9124,  ..., -1.4843, -1.5014, -1.5014]],\n\n         [[-1.7381, -1.7381, -1.7381,  ..., -1.7906, -1.7906, -1.8256],\n          [-1.6856, -1.6681, -1.6681,  ..., -1.7206, -1.7206, -1.7381],\n          [-1.6331, -1.6331, -1.6506,  ..., -1.6856, -1.6856, -1.7031],\n          ...,\n          [-1.8256, -1.8081, -1.7906,  ..., -1.7556, -1.7731, -1.7731],\n          [-1.8256, -1.8081, -1.8081,  ..., -1.7731, -1.7731, -1.7731],\n          [-1.8256, -1.8256, -1.8256,  ..., -1.7731, -1.7731, -1.7731]],\n\n         [[-1.6476, -1.6302, -1.6127,  ..., -1.7870, -1.7870, -1.7870],\n          [-1.5779, -1.5604, -1.5604,  ..., -1.7522, -1.7522, -1.7522],\n          [-1.5430, -1.5430, -1.5604,  ..., -1.7522, -1.7522, -1.7696],\n          ...,\n          [-1.8044, -1.7870, -1.7696,  ..., -1.7696, -1.7870, -1.7870],\n          [-1.7870, -1.7696, -1.7870,  ..., -1.7870, -1.7870, -1.8044],\n          [-1.7696, -1.7696, -1.7696,  ..., -1.7870, -1.8044, -1.8044]]],\n\n\n        [[[-1.6384, -1.6042, -1.5870,  ..., -1.3815, -1.3815, -1.3987],\n          [-1.5357, -1.5014, -1.4843,  ..., -1.2617, -1.2617, -1.2788],\n          [-1.4500, -1.4329, -1.4329,  ..., -1.1589, -1.1760, -1.2103],\n          ...,\n          [-1.9124, -1.8953, -1.8782,  ..., -1.4500, -1.4672, -1.4843],\n          [-1.9124, -1.8953, -1.8953,  ..., -1.4672, -1.4843, -1.5014],\n          [-1.9124, -1.9124, -1.9124,  ..., -1.4843, -1.5014, -1.5014]],\n\n         [[-1.7381, -1.7381, -1.7381,  ..., -1.7906, -1.7906, -1.8256],\n          [-1.6856, -1.6681, -1.6681,  ..., -1.7206, -1.7206, -1.7381],\n          [-1.6331, -1.6331, -1.6506,  ..., -1.6856, -1.6856, -1.7031],\n          ...,\n          [-1.8256, -1.8081, -1.7906,  ..., -1.7556, -1.7731, -1.7731],\n          [-1.8256, -1.8081, -1.8081,  ..., -1.7731, -1.7731, -1.7731],\n          [-1.8256, -1.8256, -1.8256,  ..., -1.7731, -1.7731, -1.7731]],\n\n         [[-1.6476, -1.6302, -1.6127,  ..., -1.7870, -1.7870, -1.7870],\n          [-1.5779, -1.5604, -1.5604,  ..., -1.7522, -1.7522, -1.7522],\n          [-1.5430, -1.5430, -1.5604,  ..., -1.7522, -1.7522, -1.7696],\n          ...,\n          [-1.8044, -1.7870, -1.7696,  ..., -1.7696, -1.7870, -1.7870],\n          [-1.7870, -1.7696, -1.7870,  ..., -1.7870, -1.7870, -1.8044],\n          [-1.7696, -1.7696, -1.7696,  ..., -1.7870, -1.8044, -1.8044]]],\n\n\n        [[[-1.6384, -1.6042, -1.5870,  ..., -1.3815, -1.3815, -1.3987],\n          [-1.5357, -1.5014, -1.4843,  ..., -1.2617, -1.2617, -1.2788],\n          [-1.4500, -1.4329, -1.4329,  ..., -1.1589, -1.1760, -1.2103],\n          ...,\n          [-1.9124, -1.8953, -1.8782,  ..., -1.4500, -1.4672, -1.4843],\n          [-1.9124, -1.8953, -1.8953,  ..., -1.4672, -1.4843, -1.5014],\n          [-1.9124, -1.9124, -1.9124,  ..., -1.4843, -1.5014, -1.5014]],\n\n         [[-1.7381, -1.7381, -1.7381,  ..., -1.7906, -1.7906, -1.8256],\n          [-1.6856, -1.6681, -1.6681,  ..., -1.7206, -1.7206, -1.7381],\n          [-1.6331, -1.6331, -1.6506,  ..., -1.6856, -1.6856, -1.7031],\n          ...,\n          [-1.8256, -1.8081, -1.7906,  ..., -1.7556, -1.7731, -1.7731],\n          [-1.8256, -1.8081, -1.8081,  ..., -1.7731, -1.7731, -1.7731],\n          [-1.8256, -1.8256, -1.8256,  ..., -1.7731, -1.7731, -1.7731]],\n\n         [[-1.6476, -1.6302, -1.6127,  ..., -1.7870, -1.7870, -1.7870],\n          [-1.5779, -1.5604, -1.5604,  ..., -1.7522, -1.7522, -1.7522],\n          [-1.5430, -1.5430, -1.5604,  ..., -1.7522, -1.7522, -1.7696],\n          ...,\n          [-1.8044, -1.7870, -1.7696,  ..., -1.7696, -1.7870, -1.7870],\n          [-1.7870, -1.7696, -1.7870,  ..., -1.7870, -1.7870, -1.8044],\n          [-1.7696, -1.7696, -1.7696,  ..., -1.7870, -1.8044, -1.8044]]]]), type: <class 'torch.Tensor'>, valid types: <class 'bytes'>, <class 'bytearray'>, file-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParamValidationError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-5a71d135d521>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-70-61e6b3dca61f>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(loader, model, criterion, device)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;31m# forward pass: compute predicted outputs by passing inputs to the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# fuse batch size and ncrops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mncrops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/predictor.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, data, initial_args, target_model)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mrequest_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_request_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_runtime_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mrequest_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    274\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[1;32m    275\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    557\u001b[0m         }\n\u001b[1;32m    558\u001b[0m         request_dict = self._convert_to_request_dict(\n\u001b[0;32m--> 559\u001b[0;31m             api_params, operation_model, context=request_context)\n\u001b[0m\u001b[1;32m    560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m         \u001b[0mservice_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_service_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mservice_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyphenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_convert_to_request_dict\u001b[0;34m(self, api_params, operation_model, context)\u001b[0m\n\u001b[1;32m    605\u001b[0m             api_params, operation_model, context)\n\u001b[1;32m    606\u001b[0m         request_dict = self._serializer.serialize_to_request(\n\u001b[0;32m--> 607\u001b[0;31m             api_params, operation_model)\n\u001b[0m\u001b[1;32m    608\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_client_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minject_host_prefix\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m             \u001b[0mrequest_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'host_prefix'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/botocore/validate.py\u001b[0m in \u001b[0;36mserialize_to_request\u001b[0;34m(self, parameters, operation_model)\u001b[0m\n\u001b[1;32m    295\u001b[0m                                                     operation_model.input_shape)\n\u001b[1;32m    296\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mreport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_errors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mParamValidationError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreport\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m         return self._serializer.serialize_to_request(parameters,\n\u001b[1;32m    299\u001b[0m                                                      operation_model)\n",
      "\u001b[0;31mParamValidationError\u001b[0m: Parameter validation failed:\nInvalid type for parameter Body, value: tensor([[[[-1.9124, -1.8953, -1.8439,  ..., -2.0152, -2.0494, -2.0665],\n          [-1.8953, -1.8610, -1.8610,  ..., -2.0323, -2.0665, -2.0494],\n          [-1.8268, -1.8439, -1.8610,  ..., -2.0494, -2.0837, -2.0665],\n          ...,\n          [ 0.9988,  0.9988,  1.0159,  ...,  1.1358,  1.1529,  1.1700],\n          [ 0.9988,  1.0331,  1.0331,  ...,  1.1700,  1.2043,  1.2214],\n          [ 0.9817,  0.9988,  1.0502,  ...,  1.1872,  1.2385,  1.2728]],\n\n         [[-1.8431, -1.8256, -1.7731,  ..., -1.9307, -1.9657, -1.9832],\n          [-1.7906, -1.7731, -1.7556,  ..., -1.9482, -1.9832, -1.9657],\n          [-1.6856, -1.7031, -1.7206,  ..., -1.9657, -2.0007, -1.9832],\n          ...,\n          [ 1.0805,  1.0980,  1.0980,  ...,  1.2206,  1.2381,  1.2556],\n          [ 1.0630,  1.0980,  1.0980,  ...,  1.2556,  1.2906,  1.3081],\n          [ 1.0455,  1.0630,  1.1155,  ...,  1.2731,  1.3256,  1.3606]],\n\n         [[-1.6999, -1.6650, -1.6127,  ..., -1.6650, -1.6999, -1.7173],\n          [-1.6824, -1.6476, -1.6302,  ..., -1.6824, -1.7173, -1.6999],\n          [-1.6127, -1.6302, -1.6476,  ..., -1.6999, -1.7347, -1.7173],\n          ...,\n          [ 1.2980,  1.3154,  1.3154,  ...,  1.4200,  1.4374,  1.4548],\n          [ 1.3328,  1.3677,  1.3851,  ...,  1.4548,  1.4897,  1.5071],\n          [ 1.3502,  1.3851,  1.4200,  ...,  1.4722,  1.5245,  1.5594]]],\n\n\n        [[[-1.9124, -1.8953, -1.8439,  ..., -2.0152, -2.0494, -2.0665],\n          [-1.8953, -1.8610, -1.8610,  ..., -2.0323, -2.0665, -2.0494],\n          [-1.8268, -1.8439, -1.8610,  ..., -2.0494, -2.0837, -2.0665],\n          ...,\n          [ 0.9988,  0.9988,  1.0159,  ...,  1.1358,  1.1529,  1.1700],\n          [ 0.9988,  1.0331,  1.0331,  ...,  1.1700,  1.2043,  1.2214],\n          [ 0.9817,  0.9988,  1.0502,  ...,  1.1872,  1.2385,  1.2728]],\n\n         [[-1.8431, -1.8256, -1.7731,  ..., -1.9307, -1.9657, -1.9832],\n          [-1.7906, -1.7731, -1.7556,  ..., -1.9482, -1.9832, -1.9657],\n          [-1.6856, -1.7031, -1.7206,  ..., -1.9657, -2.0007, -1.9832],\n          ...,\n          [ 1.0805,  1.0980,  1.0980,  ...,  1.2206,  1.2381,  1.2556],\n          [ 1.0630,  1.0980,  1.0980,  ...,  1.2556,  1.2906,  1.3081],\n          [ 1.0455,  1.0630,  1.1155,  ...,  1.2731,  1.3256,  1.3606]],\n\n         [[-1.6999, -1.6650, -1.6127,  ..., -1.6650, -1.6999, -1.7173],\n          [-1.6824, -1.6476, -1.6302,  ..., -1.6824, -1.7173, -1.6999],\n          [-1.6127, -1.6302, -1.6476,  ..., -1.6999, -1.7347, -1.7173],\n          ...,\n          [ 1.2980,  1.3154,  1.3154,  ...,  1.4200,  1.4374,  1.4548],\n          [ 1.3328,  1.3677,  1.3851,  ...,  1.4548,  1.4897,  1.5071],\n          [ 1.3502,  1.3851,  1.4200,  ...,  1.4722,  1.5245,  1.5594]]],\n\n\n        [[[-1.9124, -1.8953, -1.8439,  ..., -2.0152, -2.0494, -2.0665],\n          [-1.8953, -1.8610, -1.8610,  ..., -2.0323, -2.0665, -2.0494],\n          [-1.8268, -1.8439, -1.8610,  ..., -2.0494, -2.0837, -2.0665],\n          ...,\n          [ 0.9988,  0.9988,  1.0159,  ...,  1.1358,  1.1529,  1.1700],\n          [ 0.9988,  1.0331,  1.0331,  ...,  1.1700,  1.2043,  1.2214],\n          [ 0.9817,  0.9988,  1.0502,  ...,  1.1872,  1.2385,  1.2728]],\n\n         [[-1.8431, -1.8256, -1.7731,  ..., -1.9307, -1.9657, -1.9832],\n          [-1.7906, -1.7731, -1.7556,  ..., -1.9482, -1.9832, -1.9657],\n          [-1.6856, -1.7031, -1.7206,  ..., -1.9657, -2.0007, -1.9832],\n          ...,\n          [ 1.0805,  1.0980,  1.0980,  ...,  1.2206,  1.2381,  1.2556],\n          [ 1.0630,  1.0980,  1.0980,  ...,  1.2556,  1.2906,  1.3081],\n          [ 1.0455,  1.0630,  1.1155,  ...,  1.2731,  1.3256,  1.3606]],\n\n         [[-1.6999, -1.6650, -1.6127,  ..., -1.6650, -1.6999, -1.7173],\n          [-1.6824, -1.6476, -1.6302,  ..., -1.6824, -1.7173, -1.6999],\n          [-1.6127, -1.6302, -1.6476,  ..., -1.6999, -1.7347, -1.7173],\n          ...,\n          [ 1.2980,  1.3154,  1.3154,  ...,  1.4200,  1.4374,  1.4548],\n          [ 1.3328,  1.3677,  1.3851,  ...,  1.4548,  1.4897,  1.5071],\n          [ 1.3502,  1.3851,  1.4200,  ...,  1.4722,  1.5245,  1.5594]]],\n\n\n        ...,\n\n\n        [[[-1.6384, -1.6042, -1.5870,  ..., -1.3815, -1.3815, -1.3987],\n          [-1.5357, -1.5014, -1.4843,  ..., -1.2617, -1.2617, -1.2788],\n          [-1.4500, -1.4329, -1.4329,  ..., -1.1589, -1.1760, -1.2103],\n          ...,\n          [-1.9124, -1.8953, -1.8782,  ..., -1.4500, -1.4672, -1.4843],\n          [-1.9124, -1.8953, -1.8953,  ..., -1.4672, -1.4843, -1.5014],\n          [-1.9124, -1.9124, -1.9124,  ..., -1.4843, -1.5014, -1.5014]],\n\n         [[-1.7381, -1.7381, -1.7381,  ..., -1.7906, -1.7906, -1.8256],\n          [-1.6856, -1.6681, -1.6681,  ..., -1.7206, -1.7206, -1.7381],\n          [-1.6331, -1.6331, -1.6506,  ..., -1.6856, -1.6856, -1.7031],\n          ...,\n          [-1.8256, -1.8081, -1.7906,  ..., -1.7556, -1.7731, -1.7731],\n          [-1.8256, -1.8081, -1.8081,  ..., -1.7731, -1.7731, -1.7731],\n          [-1.8256, -1.8256, -1.8256,  ..., -1.7731, -1.7731, -1.7731]],\n\n         [[-1.6476, -1.6302, -1.6127,  ..., -1.7870, -1.7870, -1.7870],\n          [-1.5779, -1.5604, -1.5604,  ..., -1.7522, -1.7522, -1.7522],\n          [-1.5430, -1.5430, -1.5604,  ..., -1.7522, -1.7522, -1.7696],\n          ...,\n          [-1.8044, -1.7870, -1.7696,  ..., -1.7696, -1.7870, -1.7870],\n          [-1.7870, -1.7696, -1.7870,  ..., -1.7870, -1.7870, -1.8044],\n          [-1.7696, -1.7696, -1.7696,  ..., -1.7870, -1.8044, -1.8044]]],\n\n\n        [[[-1.6384, -1.6042, -1.5870,  ..., -1.3815, -1.3815, -1.3987],\n          [-1.5357, -1.5014, -1.4843,  ..., -1.2617, -1.2617, -1.2788],\n          [-1.4500, -1.4329, -1.4329,  ..., -1.1589, -1.1760, -1.2103],\n          ...,\n          [-1.9124, -1.8953, -1.8782,  ..., -1.4500, -1.4672, -1.4843],\n          [-1.9124, -1.8953, -1.8953,  ..., -1.4672, -1.4843, -1.5014],\n          [-1.9124, -1.9124, -1.9124,  ..., -1.4843, -1.5014, -1.5014]],\n\n         [[-1.7381, -1.7381, -1.7381,  ..., -1.7906, -1.7906, -1.8256],\n          [-1.6856, -1.6681, -1.6681,  ..., -1.7206, -1.7206, -1.7381],\n          [-1.6331, -1.6331, -1.6506,  ..., -1.6856, -1.6856, -1.7031],\n          ...,\n          [-1.8256, -1.8081, -1.7906,  ..., -1.7556, -1.7731, -1.7731],\n          [-1.8256, -1.8081, -1.8081,  ..., -1.7731, -1.7731, -1.7731],\n          [-1.8256, -1.8256, -1.8256,  ..., -1.7731, -1.7731, -1.7731]],\n\n         [[-1.6476, -1.6302, -1.6127,  ..., -1.7870, -1.7870, -1.7870],\n          [-1.5779, -1.5604, -1.5604,  ..., -1.7522, -1.7522, -1.7522],\n          [-1.5430, -1.5430, -1.5604,  ..., -1.7522, -1.7522, -1.7696],\n          ...,\n          [-1.8044, -1.7870, -1.7696,  ..., -1.7696, -1.7870, -1.7870],\n          [-1.7870, -1.7696, -1.7870,  ..., -1.7870, -1.7870, -1.8044],\n          [-1.7696, -1.7696, -1.7696,  ..., -1.7870, -1.8044, -1.8044]]],\n\n\n        [[[-1.6384, -1.6042, -1.5870,  ..., -1.3815, -1.3815, -1.3987],\n          [-1.5357, -1.5014, -1.4843,  ..., -1.2617, -1.2617, -1.2788],\n          [-1.4500, -1.4329, -1.4329,  ..., -1.1589, -1.1760, -1.2103],\n          ...,\n          [-1.9124, -1.8953, -1.8782,  ..., -1.4500, -1.4672, -1.4843],\n          [-1.9124, -1.8953, -1.8953,  ..., -1.4672, -1.4843, -1.5014],\n          [-1.9124, -1.9124, -1.9124,  ..., -1.4843, -1.5014, -1.5014]],\n\n         [[-1.7381, -1.7381, -1.7381,  ..., -1.7906, -1.7906, -1.8256],\n          [-1.6856, -1.6681, -1.6681,  ..., -1.7206, -1.7206, -1.7381],\n          [-1.6331, -1.6331, -1.6506,  ..., -1.6856, -1.6856, -1.7031],\n          ...,\n          [-1.8256, -1.8081, -1.7906,  ..., -1.7556, -1.7731, -1.7731],\n          [-1.8256, -1.8081, -1.8081,  ..., -1.7731, -1.7731, -1.7731],\n          [-1.8256, -1.8256, -1.8256,  ..., -1.7731, -1.7731, -1.7731]],\n\n         [[-1.6476, -1.6302, -1.6127,  ..., -1.7870, -1.7870, -1.7870],\n          [-1.5779, -1.5604, -1.5604,  ..., -1.7522, -1.7522, -1.7522],\n          [-1.5430, -1.5430, -1.5604,  ..., -1.7522, -1.7522, -1.7696],\n          ...,\n          [-1.8044, -1.7870, -1.7696,  ..., -1.7696, -1.7870, -1.7870],\n          [-1.7870, -1.7696, -1.7870,  ..., -1.7870, -1.7870, -1.8044],\n          [-1.7696, -1.7696, -1.7696,  ..., -1.7870, -1.8044, -1.8044]]]]), type: <class 'torch.Tensor'>, valid types: <class 'bytes'>, <class 'bytearray'>, file-like object"
     ]
    }
   ],
   "source": [
    "test(test_loader, predictor, torch.nn.CrossEntropyLoss(), device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Delete endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'PyTorchModel' object has no attribute 'delete_endpoint'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-84-46ef5e5e8b7c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'PyTorchModel' object has no attribute 'delete_endpoint'"
     ]
    }
   ],
   "source": [
    "estimator.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
